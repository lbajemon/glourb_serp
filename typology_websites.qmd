---
title: "Search Engine Pages Results : types of websites"
author: "Liolia Bajemon"
format: html
editor: visual
---

## Description

The aim of this document is to get the types of the collected websites (commercial, social media, institutional, blog, newspaper, etc.). To do this, multiple sources and tools will be used and combined:

-   Ranking of websites: the types of the most common websites will be filled by hand.

-   Wikidata: for each website URL, we will check if it corresponds to the official website of a wikidata element. If so, we will collect its type (instance_of).

-   Detailed analysis of around ten cities and completion of the types of websites by hand.

## Websites ranking

### Set-up and read data

```{r load_packages_ranking}
library(tidyverse)
library(urltools)
library(RColorBrewer)
```

```{r read_data_rank}
data_city_river = read.csv("input_data/data_city_river.csv") 
# this dataframe combines all the collected data 
data = read.csv("collected_data/corpus_distinct.csv")
names(data)[names(data) == 'domain'] = 'full_domain' # rename the "domain" column to avoid confusions 
```

### Rank the websites

```{r}
# First let's keep the websites which appear at least 10 times in the corpus
df_domain_all = data %>%
  select("full_domain") %>% 
  group_by(full_domain) %>% # group by domain 
  summarise(nb = n()) %>% # count each domain
  filter(nb > 10) 
# 568 websites are identified. Let's save the file, and identify their type by hand
write.csv(df_domain_all, "analysis/typology_websites/domain_sup10.csv")
# get the identified websites
df_domain_id = read.csv("analysis/typology_websites/domain_sup10_identified.csv")
```

### Plot most frequent domains

```{r}
# rename column
names(data)[names(data) == 'domain'] = 'full_domain' # rename the "domain" column to avoid confusions 
domain_plot = data %>% 
  mutate(domain = suffix_extract(data$full_domain)$domain) %>% 
  select("domain") %>% 
  group_by(domain) %>% 
  summarise(nb = n())  %>% 
  filter(nb > 130)

# rearrange by decreasing order
domain_plot$domain = with(domain_plot, reorder(domain, nb))

### Plot 
domain_plot %>% 
  ggplot(mapping = aes(x = domain,
                       y = nb)) +
  geom_col(position = "stack", fill = "#b3de69") +
  labs(title = "Classement des domaines du corpus",
       x = "nom du domaine",
       y = "nombre de pages") +
  coord_flip() + # flip x and y coordinates 
  theme_bw() +
  theme(legend.position = "bottom",
        plot.caption.position = "plot") 
ggsave("analysis/typology_websites/classement_domaines.png", width = 22, height = 16, unit = "cm")
write.csv(domain_plot, "analysis/typology_websites/classement_domaines.csv")
```

## Compile data analysed by hand

```{r compile_by_hand_data}

bind = function(df){
  data = read.csv(glue::glue("analysis/etude_10_villes/{df}.csv"), sep = ",")
  data = data %>% 
    select(c(link, type))
  return(data)           
}

# combine all data
by_hand = bind("ahmedabad_sabarmati_EN") %>% 
  rbind(bind(df = "ahmedabad_sabarmati_HI")) %>% 
  rbind(bind(df = "denver_south-platte_EN")) %>% 
  rbind(bind(df = "lyon_rhone_EN")) %>% 
  rbind(bind(df = "lyon_rhone_FR")) %>% 
  rbind(bind(df = "montreal_st-laurent_EN")) %>% 
  rbind(bind(df = "montreal_st-laurent_FR")) %>% 
  rbind(bind(df = "xuanwei_beipan_ZH")) %>% 
  rbind(bind(df = "xuanwei_beipan_EN")) %>% 
  rbind(bind(df = "wuhan_yangtze_ZH")) %>% 
  rbind(bind(df = "wuhan_yangtze_EN")) %>% 
  rbind(bind(df = "ufa_belaya_RU")) %>% 
  rbind(bind(df = "ufa_belaya_EN")) %>%   
  rbind(bind(df = "nha_trang_cai_EN")) %>% 
  rbind(bind(df = "nha_trang_cai_VI")) %>% 
  rbind(bind(df = "ndjamena_chari_FR")) %>% 
  rbind(bind(df = "ndjamena_chari_EN")) %>% 
  rbind(bind(df = "ndjamena_chari_AR")) %>% 
  rbind(bind(df = "abomey-calavi_oueme_FR")) %>% 
  rbind(bind(df = "abomey-calavi_oueme_EN")) %>% 
  # extract domain link
  mutate(domain = str_extract(link, "(?<=://)[^/]+")) %>% 
  distinct(domain, .keep_all = TRUE) %>% 
  mutate(type = case_when(str_detect(type, "académique|académique, scientifique") ~ "scientifique",
                          str_detect(type, "associatif, ONG") ~ "associatif",
                          str_detect(type, "commercial, entreprise|touristique, loisirs") ~ "commercial",
                          str_detect(type, "forum") ~ "institutionnel",
                          str_detect(type, "média") ~ "presse",
                          str_detect(type, "utilitaire, services") ~ "utilitaire",
                          str_detect(type, "wiki, participatif") ~ "wiki",
                    
                          TRUE ~ type)) 

write.csv(by_hand, "analysis/typology_websites/ten_byhand.csv")
```

## Get wikidata

### Set-up and read data

```{r load_packages_wiki}
library(tidyverse)
library(glitter)
library(WikidataQueryServiceR)
library(WikidataR)
library(xmlparsedata)
```

```{r read_data, warning = FALSE, message = FALSE}
# retrieve data
df_hl = read.csv("collected_data/hl/df_scrap_tokens_all_hl.csv")
df_en = read.csv("collected_data/english/scrap_data_river_en.csv")

# prepare data
data_hl = df_hl %>%
  # get the url by extracting it from the displayed link
  separate(displayed_link, c("website", "page"), sep = " › ") %>% 
  # remove the 2nd part which we will not use
  select(-page) %>% 
  # for some websites, the local domain is given (e.g. facebook or wikipedia) so we need to homogeneize it 
  mutate(website = ifelse(str_detect(domain, "wikipedia"), "https://wikipedia.org", website)) %>% 
  # for facebook, linkedin we will check if the word is contained in the domain column because some of the supposed url do not correspond to url (e.g. "2,3 k+ followers")
  mutate(website = ifelse(str_detect(domain, "facebook"), "https://www.facebook.com", website)) %>%
  mutate(website = ifelse(str_detect(domain, "linkedin"), "https://www.linkedin.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "instagram"), "https://www.instagram.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "twitter"), "https://twitter.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "tiktok"), "https://www.tiktok.com", website)) %>%   
  mutate(website = ifelse(str_detect(domain, "youtube"), "https://www.youtube.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "reddit"), "https://www.reddit.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "pinterest"), "https://www.pinterest.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "medium"), "https://medium.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "substack"), "https://substack.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "soundcloud"), "https://soundcloud.com", website)) %>% 
  # only keep distinct websites to reduce the number of queries
  distinct(website) %>% 
  # define a number for each website (i.e. an ID) 
  dplyr::mutate(i = row_number())

data_en = df_en %>%
  # get the url by extracting it from the displayed link
  separate(displayed_link, c("website", "page"), sep = " › ") %>% 
  # remove the 2nd part which we will not use
  select(-page) %>% 
  # for some websites, the local domain is given (e.g. facebook or wikipedia) so we need to homogeneize it 
  mutate(website = ifelse(str_detect(domain, "wikipedia"), "https://wikipedia.org", website)) %>% 
  # for facebook, linkedin we will check if the word is contained in the domain column because some of the supposed url do not correspond to url (e.g. "2,3 k+ followers")
  mutate(website = ifelse(str_detect(domain, "facebook"), "https://www.facebook.com", website)) %>%
  mutate(website = ifelse(str_detect(domain, "linkedin"), "https://www.linkedin.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "instagram"), "https://www.instagram.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "twitter"), "https://twitter.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "tiktok"), "https://www.tiktok.com", website)) %>%   
  mutate(website = ifelse(str_detect(domain, "youtube"), "https://www.youtube.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "reddit"), "https://www.reddit.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "pinterest"), "https://www.pinterest.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "medium"), "https://medium.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "substack"), "https://substack.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "soundcloud"), "https://soundcloud.com", website)) %>% 
  mutate(website = ifelse(str_detect(domain, "airbnb"), "https://www.airbnb.com", website)) %>% 
  # only keep distinct websites to reduce the number of queries
  distinct(website) %>% 
  # define a number for each website (i.e. an ID) 
  dplyr::mutate(i = row_number())
```

### Make queries

We will use the glitter R package to send SPARQL queries to collect Wikidata elements (Vaudor & Salmon 2023). For each of our websites, we will check if there is an associated element in Wikidata. This will enable us to characterize the website's type (social media, newspaper, *etc.*) as well as its location (country or city).

```{r def_get_websites}

get_websites_attributes_hl = function(website, i){
  triplet = paste0("?item wdt:P856 <",website,">")
  result = spq_init() %>%
    spq_add(triplet) %>% 
    spq_add("?item wdt:P31 ?type", .required = FALSE) %>%
    spq_perform()
  write.csv(x = result, file = glue::glue("analysis/critical/wikidata/hl/{i}.csv"))
}

get_websites_attributes_en = function(website, i){
  triplet = paste0("?item wdt:P856 <",website,">")
  result = spq_init() %>%
    spq_add(triplet) %>% 
    spq_add("?item wdt:P31 ?type", .required = FALSE) %>%
    spq_perform()
  write.csv(x = result, file = glue::glue("analysis/typology/wikidata/en/{i}.csv"))
}
```

Now let's run the function and compile the results:

```{r run_get_websites}
### for local languages ###
data_hl %>% 
  mutate(data = purrr::map2(website, i, get_websites_attributes_hl))

# compile the resulting data
  # create empty df
compile = data.frame(
  X = as.integer(),
  item = as.character(),
  type = as.character(),
  id = as.integer()
)

for(i in 1:nrow(data_hl)){
  df_i = read.csv(glue::glue("analysis/critical/wikidata/hl/{i}.csv")) %>% 
    mutate(id = i)
  if ("type" %in% colnames(df_i)) {
    df_i$type <- df_i$type
  } else {
    df_i$type <- NA_character_
  }
  compile = compile %>% 
    rbind(df_i)
}

write.csv(compile, "analysis/critical/wikidata/wikidata_compile_hl.csv")

### for english ###
data_en %>% 
  mutate(data = purrr::map2(website, i, get_websites_attributes_en))

# compile the resulting data
  # create empty df
compile_en = data.frame(
  X = as.integer(),
  item = as.character(),
  type = as.character(),
  id = as.integer()
)

for(i in 1:nrow(data_en)){
  df_i = read.csv(glue::glue("analysis/critical/wikidata/en/{i}.csv")) %>% 
    mutate(id = i)
  if ("type" %in% colnames(df_i)) {
    df_i$type <- df_i$type
  } else {
    df_i$type <- NA_character_
  }
  compile_en = compile_en %>% 
    rbind(df_i)
}

write.csv(compile_en, "analysis/critical/wikidata/wikidata_compile_en.csv")
```

### Join with data

```{r}
compile_en = read.csv("analysis/typology/wikidata/wikidata_compile_en.csv")
compile_hl = read.csv("analysis/typology/wikidata/wikidata_compile_hl.csv")

# now combine the collected data
types = bind_rows(compile_hl, compile_en) %>% 
  # keep only distinct types
  distinct(type, .keep_all = TRUE)
write.csv(types, "analysis/typology/wikidata/distinct_types.csv", row.names = FALSE)
# the labels are retrieved by hand and classified in bigger categories
# read updated data
category = read.csv("analysis/typology/wikidata/types.csv") %>% 
  select(-c(X.1, X, id, item))

compile = bind_rows(compile_hl, compile_en) %>% 
  # add collected labels to the combined data
  left_join(category, by = "type") %>% 
  select(-c(X.1, X)) %>% 
  # some websites have multiple labels (types)
  # let's identify them 
  distinct(item, cat, .keep_all = TRUE) %>% 
  write.csv("analysis/typology/wikidata/compile_types.csv")
# and choose a category by hand

# join with website url
compile_en = read.csv("analysis/typology/wikidata/compile_types_en.csv") %>% 
  left_join(data_en, by = c("id"="i")) %>% 
  select(c(cat, website)) %>% 
  distinct(website, .keep_all = TRUE) 
compile_hl = read.csv("analysis/typology/wikidata/compile_types_hl.csv", sep = ";") %>% 
  left_join(data_hl, by = c("id"="i")) %>% 
  select(c(cat, website)) %>% 
  distinct(website, .keep_all = TRUE)

write.csv(compile_en, "analysis/typology_websites/wikidata_en.csv")
write.csv(compile_hl, "analysis/typology_websites/wikidata_hl.csv")
```

## Identify types of websites

We will use the data collected beforehand to add it to our SERP results. We consider that the most trustworthy types are the ones obtained from the ranking and from the analysis by hand, because the types have been determined manually. The less trustworthy results are the one obtained from the Wikidata queries, because they have been determined semi-automatically, and are therefore more subject to errors.

```{r retrieve_data}
data = read.csv("collected_data/corpus_distinct.csv")

# the wikidata results are checked by hand
wiki_data_en = read.csv("analysis/typology_websites/wikidata_en_checked.csv") %>% 
  mutate(domain = str_remove(website, "^https?://")) %>% 
  mutate(type = case_when(
    str_detect(cat, "académique") ~ "scientifique",
    str_detect(cat, "associatif, ONG") ~ "associatif",
    str_detect(cat, "commercial, entreprise|touristique, loisirs|entreprise, commerce") ~ "commercial",
    str_detect(cat, "encyclopédie$") ~ "encyclopédie, base de données",
    str_detect(cat, "média") ~ "presse",
    str_detect(cat, "utilitaire, services") ~ "utilitaire",
    str_detect(cat, "wiki, participatif") ~ "wiki",
    TRUE ~ cat)) %>% 
  select(c("domain", "type"))

wiki_data_hl = read.csv("analysis/typology_websites/wikidata_hl_checked.csv") %>% 
  mutate(domain = str_remove(website, "^https?://")) %>% 
   mutate(type = case_when(
     str_detect(cat, "académique") ~ "scientifique",
     str_detect(cat, "associatif, ONG|ONG, association") ~ "associatif",
     str_detect(cat, "commercial, entreprise|touristique, loisirs|entreprise, commerce") ~ "commercial",
     str_detect(cat, "encyclopédie$") ~ "encyclopédie, base de données",
     str_detect(cat, "média|journal") ~ "presse",
     str_detect(cat, "utilitaire, services") ~ "utilitaire",
     str_detect(cat, "wiki, participatif") ~ "wiki",
     TRUE ~ cat)) %>% 
  select(c("domain", "type"))

wiki2 = wiki_data_en %>% 
  rbind(wiki_data_hl)
names(wiki2)[names(wiki2) == 'type'] = 'type_wiki'

ten_byhand = read.csv("analysis/typology_websites/ten_byhand.csv") %>% 
    select(c("domain", "type"))
names(ten_byhand)[names(ten_byhand) == 'type'] = 'type_ten'

rank_byhand = read.csv("analysis/typology_websites/domain_sup10_identified.csv") %>% 
  mutate(domain = full_domain) %>% 
  select(c("domain", "type"))
names(rank_byhand)[names(rank_byhand) == 'type'] = 'type_hand'
```

### Combine everything

```{r combine}
df = data %>% 
  left_join(rank_byhand, by = "domain") %>% 
  left_join(ten_byhand, by = "domain") %>% 
  left_join(wiki2, by = "domain") %>% 
  mutate(type = coalesce(type_hand, type_ten, type_wiki)) %>% 
  # add some categories by hand 
  mutate(type = case_when(
    str_detect(domain, "news|radio|livejournal|gmw|.tv|daily|bbc") & is.na(type) ~ "presse",
    str_detect(domain, "gov") & is.na(type) ~ "institutionnel",
    str_detect(domain, "wordpress") & is.na(type) ~ "magazine, blog",
    str_detect(domain, "facebook|twitter|instagram|flickr|youtube|tiktok|pinterest|reddit") & is.na(type)  ~ "réseau social",
    str_detect(domain, "wikipedia|dbpedia|pedia|wiki|unionpedia|vikidia") & is.na(type)  ~ "wiki",
    str_detect(domain, "skyscanner|expedia|airbnb|tripadvisor|amazon") & is.na(type) ~ "commercial",
    str_detect(domain, "blogspot") & is.na(type) ~ "magazine, blog",
    str_detect(domain, "books.google.") & is.na(type) ~ "livre, rapport",
    str_detect(domain, ".edu") & is.na(type) ~ "éducatif",
    str_detect(domain, ".org") & is.na(type) ~ "associatif",
    TRUE ~ type)) 

df_non_na = df %>% 
    filter(!is.na(tokenized_text))

nb_id = df %>% 
  filter(!is.na(type)) %>% 
  filter(!is.na(tokenized_text))
non_id = df %>% 
  filter(is.na(type)) %>% 
  select("link") %>% 
  group_by(link) %>% 
  distinct()

write.csv(df, "analysis/typology_websites/corpus_website_types.csv")
write.csv(df_non_na, "analysis/typology_websites/corpus_distinct_website_types.csv")
```

### Plots

```{r plot}
# prepare data 
df_plot = df %>% 
  select(c("query", "type")) %>% 
  mutate(type = trimws(type)) %>% 
  group_by(query, type) %>% 
  summarise(nb = n()) %>% 
  filter(!is.na(type)) 
df_total = df_plot %>% 
  group_by(type) %>% 
  summarise(total = sum(nb)) 
df_plot = df_plot %>% 
  left_join(df_total, by = "type") %>% 
  mutate(type = fct_reorder(type, total)) 
  
# plot
df_plot %>% 
  ggplot(aes(x = nb, y = type)) + 
  geom_col(aes(fill = query), width = 0.7) +
  scale_fill_manual(values = c("#fa9fb5", "#a6d854"),
                    labels = c("Anglais", "Langues locales")) +
  labs(x = "Nombre de pages",
       y = NULL,
       fill = "Langue de requête",
       title = "Les types de sites web du corpus",
       subtitle = "N = 30,289") +
  theme_bw()
ggsave("analysis/typology_websites/types_sites.png", width = 20, height = 10, units = "cm")
```

Now let's combine two plots showing the percentages of each type for English and local queries:

```{r combine_plots}
library(patchwork)

# prepare data 
data_en = df_plot %>% 
  filter(query == "english") %>% 
  mutate(perc = nb/sum(nb)*100)

plot_en = data_en %>% 
  mutate(type = fct_reorder(type, perc)) %>% 
  ggplot(aes(x = type, y = perc)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8, fill = "#fa9fb5") + 
  coord_flip() + # flip x and y coordinates 
  labs(x = NULL,
       y = "%",
       title = "Les requêtes en anglais") +
  theme_bw()

data_hl = df_plot %>% 
  filter(query == "hl") %>% 
  mutate(perc = nb/sum(nb)*100)

plot_hl = data_hl %>% 
  mutate(type = fct_reorder(type, perc)) %>% 
  ggplot(aes(x = type, y = perc)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8, fill = "#b3de69") + 
  coord_flip() + # flip x and y coordinates 
  labs(x = NULL,
       y = "%",
       title = "Les requêtes dans les langues locales") +
  theme_bw() 

combined_plot = plot_en + plot_hl
ggsave("analysis/typology_websites/type_repartition.png", width = 28, height = 15, units = "cm")
```
