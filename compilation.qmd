---
title: "Search Engine Pages Results: Data compilation"
author: "Liolia Bajemon"
format: html
editor: visual
---

### Description

Thanks to the Value SERP API ([access link](https://get.valueserp.com/try-it-free/)), data relative to Google searches have been gathered (see [here](https://github.com/lbajemon/glourb_serp/blob/main/R/get_data.qmd)) and lemmatised (see [here](https://github.com/lbajemon/glourb_serp/blob/main/R/lemmatisation.qmd)). This document aims to compile the data in one single table. This table will be used for further analysis (clustering, topic modelling, specificity score, *etc.*).

### Set-up

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(magrittr)
library(tidyr)
library(plyr)
library(purrr)
library(stringr)
library(tokenizers)
library(lexicon)
library(dplyr)
```

```{r read_data}
data_city_river = read.csv("input_data/data_city_river.csv")
```

### Compilation of results from the queries ran in English

First, let's make tables with the data from the query ran in English.

This table contains all the tokens from all the snippets combined in a single dataframe:

##### Combine tokens

```{r combine_tokens_en}

# Combine all tokens in a single dataframe
df = data.frame(data_city_river[,c(1,2, 21)]) %>% 
  mutate(path = paste0("collected_data/english/tokens/tokens_", fid, ".csv")) %>%  # indicate the path for finding the file for each city
  mutate(data = map(path, read.csv)) %>% # read the data for each city
  unnest(data) %>%  # create a row for each token for each city, i.e. over 287 000 rows
  mutate(snippet_id = paste0(fid, "_", position)) # create a unique id for each snippet
df = df[,c(1:3,5:11,29,30,34,35,43)]
write.csv(df, "collected_data/english/df_tokens.csv", row.names = FALSE)
```

##### Combine tokenized snippets

The table below contains all the original snippets, the translated snippets and the tokenized snippets for each city. For example, the snippet "*The wetland vegetations in Luliang Prefecture are concentrately distributed in the basins of Sanchuan River*" becomes "*wetland vegetation prefecture concentrately distribute basin wucheng*" with the locations and "*wetland vegetation prefecture concentrately distribute basin*" without the locations.

```{r combine_snippets}
# Now let's collapse the tokens for each snippet, i.e. create a "tokenized" snippet
# one including locations 
tokenized_snippets = ddply(df, "snippet_id", summarize, tokenized = paste(token, collapse = " "))
# one excluding locations
df_without_loc = subset(df, df$type != "location")
tokenized_snippets_loc = ddply(df_without_loc, "snippet_id", summarize, tokenized = paste(token, collapse = " "))

# now put the resulting tokenized sentences in a df containing all of the snippets 
df_compile = data.frame(data_city_river[,c(1,2)]) %>% 
  mutate(path = paste0("collected_data/english/clean_data/clean_", fid, ".csv")) %>% 
  mutate(data = map(path, read.csv)) %>% # read the data for each city
  unnest(data) %>%  # create a row for each token for each city, i.e. over 287 000 rows
  mutate(snippet_id = paste0(fid, "_", position)) %>% # create a unique id for each snippet 
  # let's add the tokenized snippets:
  left_join(tokenized_snippets, by = "snippet_id", keep = FALSE) %>% 
  left_join(tokenized_snippets_loc, by = "snippet_id", keep = FALSE)

# simplify the resulting dataframe and save it
df_compile = df_compile[,c(1:2,4:10,28,36:38)]
colnames(df_compile) = c("fid","urban_aggl","position","title","link","domain","displayed_link","snippet","english","trans_snippet","snippet_id","tokenized","tokenized_noloc")
write.csv(df_compile, "collected_data/english/df_tokenized_snippets.csv", row.names = FALSE)

    # fid: identifier of the city
    # urban_aggl: usual English name of the city
    # position: position of the snippet on the pages result
    # title: title of the page
    # link: link of the page
    # domain: domain of the page
    # displayed_link: displayed link of the page
    # snippet: original snippet as displayed on the pages result, not necessarily in English
    # trans_snippet: snippet in English
    # english: boolean, indicates if the original snippet was in English
    # snippet_id: unique identifier of the snippet, composed of the FID of the city with the position of the snippet
    # tokenized: the tokenized snippet with the locations
    # tokenized_noloc: the tokenized snippet without the locations
```

Now, we have dataframe with the all the snippets combined, just like this (n_snippet â‰¤ 98 and n_city = 370):

![](https://github.com/lbajemon/glourb_serp/blob/main/figures/snippets_table.png){fig-alt="Extract of the combined table"}

### Compilation of results from the queries ran in the local languages

Because every city has at least one local language, but sometimes not more, we will compile the data as follows:

-   [Compilation 1:]{.underline} data found for queries in the first local language (hl1) (concerns all cities)

-   [Compilation 2:]{.underline} data found for queries in the second local language (hl2) (does not concern all cities)

-   [Compilation 3:]{.underline} data found for queries in the third local language (hl3) (does not concern all cities)

-   [Compilation 4:]{.underline} data found for queries in the fourth local language (hl4) (does not concern all cities)

-   [Compilation 5:]{.underline} data found for queries in the third local language (hl5) (does not concern all cities)

-   [Compilation 6:]{.underline} data found for queries in the third local language (hl6) (does not concern all cities)

-   [Compilation 7:]{.underline} combine all data found for queries in all local languages (hl1 to hl6) (concerns all cities and combines all the previous compilations)

We will use the same approach as for the queries in English.

#### Compilation 1: in the 1st local language

##### Combine tokens

```{r compile_tokens_hl1}

# get path 
df = data.frame(data_city_river[,c(1,2,7,8,21)]) %>% 
  mutate(path = paste0("collected_data/hl/tokens/tokens_", fid, "_", hl1, ".csv")) %>% # indicate the path for finding the file for each city
  # check if data exists
  mutate(path = case_when(file.exists(path) ~ path,
  # if there is no data and the country is the USA, then data can be found in another folder
                          !file.exists(path) & gl == "us" & hl1 != "no data" ~ paste0("collected_data/english/tokens/tokens_", fid, ".csv"),
  # else write "no data"
                          !file.exists(path) & gl != "us" ~ NA))

# keep count of cities where there is no data
no_data = df %>% 
  filter(is.na(path)) %>% 
  select(-c("path")) 
write.csv(no_data, "collected_data/hl/no_data_hl1.csv", row.names = FALSE)

# read files
df = df %>% 
  filter(!is.na(path)) %>% # remove cities for which there is no data (n = 7) 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% # create a row for each token for each city
  mutate(snippet_id = paste0(fid, "_", position)) # create a unique id for each snippet
df = df[,c(1:5,8:13,25,30,31,48)]
write.csv(df, "collected_data/hl/df_tokens_hl1.csv", row.names = FALSE)
```

Results have been found for 365 queries, out of 372.

##### Combine tokenized snippets

```{r combine_snippets_hl1}
# Now let's collapse the tokens for each snippet, i.e. create a "tokenized" snippet
# one including locations, i.e. all tokens
tokenized_snippets = ddply(df, "snippet_id", summarize, tokenized = paste(token, collapse = " "))
# one excluding locations
df_without_loc = subset(df, df$type != "location")
tokenized_snippets_loc = ddply(df_without_loc, "snippet_id", summarize, tokenized = paste(token, collapse = " "))

# now put the resulting tokenized sentences in a df containing all of the snippets 
df_compile = data.frame(data_city_river[,c(1,2,7,8,21)]) %>% 
  mutate(path = paste0("collected_data/hl/clean_data/clean_", fid, "_", hl1, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl1 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA)) %>% 
  filter(!is.na(path)) %>% # remove cities for which there is no data
  mutate(data = map(path, read.csv)) %>% # read the data for each city
  unnest(data) %>% # create a row for each token for each city, i.e. over 287 000 rows
  mutate(snippet_id = paste0(fid, "_", position)) %>% # create a unique id for each snippet 
  # let's add the tokenized snippets:
  left_join(tokenized_snippets, by = "snippet_id", keep = FALSE) %>% 
  left_join(tokenized_snippets_loc, by = "snippet_id", keep = FALSE)

# simplify the resulting dataframe and save it
df_compile = df_compile[,c(1:5,8:13,25,42:44)]

colnames(df_compile) = c("fid","urban_aggl","gl","hl","river","position","title","link","domain","displayed_link","snippet","trans_snippet","snippet_id","tokenized","tokenized_noloc")
write.csv(df_compile, "collected_data/hl/df_tokenized_hl1.csv", row.names = FALSE)
```

#### Compilation 2: in the 2nd local language

##### Combine tokens

```{r compile_hl2}
## Combine all tokens in a single dataframe

# get path 
df = data.frame(data_city_river[,c(1,2,7,9,21)]) %>% 
  mutate(path = paste0("collected_data/hl/tokens/tokens_", fid, "_", hl2, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl2 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA))

# keep count of cities where there is no data (when applicable, i.e. a city without a second local language is not counted)
no_data = df %>% 
  filter(is.na(path) & hl2 != "no data") %>% 
  select(-c("path"))
write.csv(no_data, "collected_data/hl/no_data_hl2.csv", row.names = FALSE)

# read files
df = df %>% 
  subset(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% # create a row for each token for each city
  mutate(snippet_id = paste0(fid, "_", position)) # create a unique id for each snippet
df = df[,c(1:5,8:13,26,31,32,46)]
write.csv(df, "collected_data/hl/df_tokens_hl2.csv", row.names = FALSE)
```

##### Combine tokenized snippets

```{r combine_snippets_hl2}
tokenized_snippets = ddply(df, "snippet_id", summarize, tokenized = paste(token, collapse = " "))
df_without_loc = subset(df, df$type != "location")
tokenized_snippets_loc = ddply(df_without_loc, "snippet_id", summarize, tokenized = paste(token, collapse = " "))

df_compile = data.frame(data_city_river[,c(1,2,7,9,21)]) %>% 
  mutate(path = paste0("collected_data/hl/clean_data/clean_", fid, "_", hl2, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl2 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA)) %>% 
  filter(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>%
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) %>% 
  left_join(tokenized_snippets, by = "snippet_id", keep = FALSE) %>% 
  left_join(tokenized_snippets_loc, by = "snippet_id", keep = FALSE)

df_compile = df_compile[,c(1:5,8:13,26,40:42)]

colnames(df_compile) = c("fid","urban_aggl","gl","hl","river","position","title","link","domain","displayed_link","snippet","trans_snippet","snippet_id","tokenized","tokenized_noloc")
write.csv(df_compile, "collected_data/hl/df_tokenized_hl2.csv", row.names = FALSE)
```

#### Compilation 3: in the 3rd local language

##### Combine tokens

```{r compile_hl3}

df = data.frame(data_city_river[,c(1,2,7,10,21)]) %>% 
  mutate(path = paste0("collected_data/hl/tokens/tokens_", fid, "_", hl3, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl3 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA))

no_data = df %>% 
  filter(is.na(path) & hl3 != "no data") %>% 
  select(-c("path"))
write.csv(no_data, "collected_data/hl/no_data_hl3.csv", row.names = FALSE)

# read files
df = df %>% 
  subset(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) 
df = df[,c(1:5,8:13,27,32,33,46)]
write.csv(df, "collected_data/hl/df_tokens_hl3.csv", row.names = FALSE)
```

##### Combine tokenized snippets

```{r combine_snippets_hl3}
tokenized_snippets = ddply(df, "snippet_id", summarize, tokenized = paste(token, collapse = " "))
df_without_loc = subset(df, df$type != "location")
tokenized_snippets_loc = ddply(df_without_loc, "snippet_id", summarize, tokenized = paste(token, collapse = " "))

df_compile = data.frame(data_city_river[,c(1,2,7,10,21)]) %>% 
  mutate(path = paste0("collected_data/hl/clean_data/clean_", fid, "_", hl3, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl3 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA)) %>% 
  filter(!is.na(path)) %>%
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) %>% 
  left_join(tokenized_snippets, by = "snippet_id", keep = FALSE) %>% 
  left_join(tokenized_snippets_loc, by = "snippet_id", keep = FALSE)

df_compile = df_compile[,c(1:5,8:13,27,40:42)]

colnames(df_compile) = c("fid","urban_aggl","gl","hl","river","position","title","link","domain","displayed_link","snippet","trans_snippet","snippet_id","tokenized","tokenized_noloc")
write.csv(df_compile, "collected_data/hl/df_tokenized_hl3.csv", row.names = FALSE)
```

#### Compilation 4: in the 4th local language

##### Combine tokens

```{r compile_hl4}

df = data.frame(data_city_river[,c(1,2,7,11,21)]) %>% 
  mutate(path = paste0("collected_data/hl/tokens/tokens_", fid, "_", hl4, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl4 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA))

no_data = df %>% 
  filter(is.na(path) & hl4 != "no data") %>% 
  select(-c("path"))
write.csv(no_data, "collected_data/hl/no_data_hl4.csv", row.names = FALSE)

df = df %>% 
  subset(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) 

df = df[,c(1:5,8:13,27,32,33,43)]
write.csv(df, "collected_data/hl/df_tokens_hl4.csv", row.names = FALSE)
```

##### Combine tokenized snippets

```{r combine_snippets_hl4}
tokenized_snippets = ddply(df, "snippet_id", summarize, tokenized = paste(token, collapse = " "))
df_without_loc = subset(df, df$type != "location")
tokenized_snippets_loc = ddply(df_without_loc, "snippet_id", summarize, tokenized = paste(token, collapse = " "))

df_compile = data.frame(data_city_river[,c(1,2,7,11,21)]) %>% 
  mutate(path = paste0("collected_data/hl/clean_data/clean_", fid, "_", hl4, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl4 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA)) %>% 
  filter(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) %>% 
  left_join(tokenized_snippets, by = "snippet_id", keep = FALSE) %>% 
  left_join(tokenized_snippets_loc, by = "snippet_id", keep = FALSE)

df_compile = df_compile[,c(1:5,8:13,27,37:39)]

colnames(df_compile) = c("fid","urban_aggl","gl","hl","river","position","title","link","domain","displayed_link","snippet","trans_snippet","snippet_id","tokenized","tokenized_noloc")
write.csv(df_compile, "collected_data/hl/df_tokenized_hl4.csv", row.names = FALSE)
```

#### Compilation 5: in the 5th local language

##### Combine tokens

```{r compile_hl5}

df = data.frame(data_city_river[,c(1,2,7,12,21)]) %>% 
  mutate(path = paste0("collected_data/hl/tokens/tokens_", fid, "_", hl5, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl5 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA))

no_data = df %>% 
  filter(is.na(path) & hl5 != "no data") %>% 
  select(-c("path"))
write.csv(no_data, "collected_data/hl/no_data_hl5.csv", row.names = FALSE)

df = df %>% 
  subset(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) 

df = df[,c(1:5,8:13,20,25:27)]
write.csv(df, "collected_data/hl/df_tokens_hl5.csv", row.names = FALSE)
```

##### Combine tokenized snippets

```{r combine_snippets_hl5}
tokenized_snippets = ddply(df, "snippet_id", summarize, tokenized = paste(token, collapse = " "))
df_without_loc = subset(df, df$type != "location")
tokenized_snippets_loc = ddply(df_without_loc, "snippet_id", summarize, tokenized = paste(token, collapse = " "))

df_compile = data.frame(data_city_river[,c(1,2,7,12,21)]) %>% 
  mutate(path = paste0("collected_data/hl/clean_data/clean_", fid, "_", hl5, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl5 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA)) %>% 
  filter(!is.na(path)) %>% 
  mutate(data = map(path, read.csv)) %>% 
  unnest(data) %>% 
  mutate(snippet_id = paste0(fid, "_", position)) %>% 
  left_join(tokenized_snippets, by = "snippet_id", keep = FALSE) %>% 
  left_join(tokenized_snippets_loc, by = "snippet_id", keep = FALSE)

df_compile = df_compile[,c(1:5,8:13,20:23)]

colnames(df_compile) = c("fid","urban_aggl","gl","hl","river","position","title","link","domain","displayed_link","snippet","trans_snippet","snippet_id","tokenized","tokenized_noloc")
write.csv(df_compile, "collected_data/hl/df_tokenized_hl5.csv", row.names = FALSE)
```

#### Compilation 6: in the 6th local language

##### Combine tokens

```{r compile_hl6}

df = data.frame(data_city_river[,c(1,2,7,13,21)]) %>% 
  mutate(path = paste0("collected_data/hl/tokens/tokens_", fid, "_", hl6, ".csv")) %>% 
  mutate(path = case_when(file.exists(path) ~ path,
                          !file.exists(path) & gl == "us" & hl6 != "no data" ~ paste0("collected_data/english/clean_data/clean_", fid, ".csv"),
                          !file.exists(path) & gl != "us" ~ NA)) 

no_data = df %>% 
  filter(is.na(path) & hl6 != "no data") %>% 
  select(-c("path"))
write.csv(no_data, "collected_data/hl/no_data_hl6.csv", row.names = FALSE)

df = df %>% 
  filter(!is.na(path))
# No data available (0 obs.)
```

No data is available for the cities which have a 6th language (2 cities in South Africa).

#### Compilation 7: Combine all results in local languages 

##### Combine all tokens

```{r combine_all_tokens_hl}

df1 = read.csv("collected_data/hl/df_tokens_hl1.csv") %>% 
  rename_at("hl1", ~"hl")
df2 = read.csv("collected_data/hl/df_tokens_hl2.csv") %>% 
  rename_at("hl2", ~"hl")
df3 = read.csv("collected_data/hl/df_tokens_hl3.csv") %>% 
  rename_at("hl3", ~"hl")
df4 = read.csv("collected_data/hl/df_tokens_hl4.csv") %>% 
  rename_at("hl4", ~"hl")
df5 = read.csv("collected_data/hl/df_tokens_hl5.csv") %>% 
  rename_at("hl5", ~"hl")

df = df1 %>% 
  rbind(df2, df3, df4, df5) %>% 
  # modify snippet_id to integrate the language
  mutate(snippet_id = paste0(fid, "_", hl, "_", position))

write.csv(df, "collected_data/hl/df_tokens_all_hl.csv", row.names = FALSE)
```

##### Combine all tokenized snippets

```{r combine_all_snippets_hl}

df = read.csv("collected_data/hl/df_tokenized_hl1.csv") %>% 
  rbind(read.csv("collected_data/hl/df_tokenized_hl2.csv")) %>% 
  rbind(read.csv("collected_data/hl/df_tokenized_hl3.csv")) %>% 
  rbind(read.csv("collected_data/hl/df_tokenized_hl4.csv")) %>% 
  rbind(read.csv("collected_data/hl/df_tokenized_hl5.csv")) %>% 
  mutate(snippet_id = paste0(fid, "_", hl, "_", position))

write.csv(df, "collected_data/hl/df_tokenized_all.csv", row.names = FALSE)
```
