---
title: "Search Engine Pages Results: Clustering"
format: html
editor: visual
author: "Liolia Bajemon"
---

### Description

The aim of this document is to do a clusterisation on a corpus.

Our corpus consists of snippets (or "rich snippets", i.e. the first lines extracted from a search engine pages results - SERP) extracted from multiple queries on Google. For 303 combinations of a city and its river(s), a {rivername} AND {cityname} type query was ran, in each official language of the given city (for more details, see <https://github.com/lbajemon/glourb_serp>). Each query was ran 4 times over 4 months, to reduce the eventual variability due to special events.

This method uses the [rainette package](https://juba.github.io/rainette/), which is based on a variant of the Reinert textual clustering method. This classification method is used for short text segments, which applies to our case, the snippets being approximately thirty words long.

### Set-up

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(magrittr)
library(tidyr)
library(plyr)
library(purrr)
library(stringr)
library(mixr)
library(tokenizers)
library(lexicon)
library(dplyr)
library(quanteda)
library(rainette)
library(rlist)
set.seed(1204) # set seed to get reproductible results
```

### 1. Read corpus

First, let's import our dataset and convert it to a corpus using the quanteda package:

```{r def_read_corpus}
read_corpus = function(df, text_field, docid_field){
  my_corpus = quanteda::corpus(df, docid_field, text_field) 
  # now make tokens
  tok_serp = tokens(my_corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, split_hyphens = TRUE) # tokenization and removal of diverse items 
  }
```

The tokenized snippets are already quite short, so we can skip the step of splitting the text in segments (using `split_segments`) and move on to the calculation of the document-feature matrix (dfm). The dfm is a mathematical matrix describing for each term, its frequency in each document. In rows are the documents (i.e. one row per snippet) and in columns are the terms.

### 2. Clustering

#### a. Calculate the DFM

```{r def_calc_dtm}
calc_dtm = function(tokens){
  # calculate the DFM
  dfmatrix = dfm(tokens, remove_padding = TRUE) # remove NA or empty values
  # remove feature which appear in a given number of documents 
  # and at least a given number of times 
  dfmatrix <- dfm_trim(dfmatrix, min_docfreq = 10, min_termfreq = 200)
}
```

#### b. Apply clustering

```{r dhc, warning = FALSE, message = FALSE}
run_dhc = function(df, dfmatrix, n_class, min_members, corpus){

  # df: original dataset
  # dfmatrix: the document feature matrix calculated beforehand
  # n_class: number of clusters
  # min_members: do not split the cluster again if it contains less than 500 members
  # corpus: corpus imported beforehand
  
  k = n_class
  min_split_members = min_members

  dhc = rainette(dfmatrix, k, min_split_members = min_split_members, min_segment_size = 0)

# plot
  plot = rainette_plot(
    dhc, dfmatrix, k,
    n_terms = 20,
    free_scales = FALSE,
    measure = "chi2",
    show_negative = FALSE,
    text_size = 12
  )
  ggsave(glue::glue("4_clustering/clustering_k{k}.png"), plot, width = 500, height = 250, units = "mm", limitsize = FALSE)

  # save the results (i.e. most frequent terms for each cluster)
  groups = cutree_rainette(dhc, k = n_class)
  groups_stats = rainette_stats(groups, dfmatrix)
  list.save(groups_stats, glue::glue("4_clustering/resultats_statistiques_k{k}.rds"))
  
  # for each element of the corpus (i.e. each snippet)
  # write its class according to the clustering
  corpus$cluster = cutree_rainette(dhc, k)
  # create a df indicating the cluster for each doc 
  docvar = docvars(corpus) %>% 
    mutate(seg_id = paste0(fid, "_", position)) %>% 
    # seg_id: unique identifier of each segment 
    subset(select = -c(1:(ncol(docvars(corpus))-1)))
  # join with our original dataset
  df_clusters = df %>% 
    left_join(docvar, by = "seg_id", keep = FALSE)
# and save the result
  write.csv(df_clusters, glue::glue("4_clustering/classified_snippets_k{k}.csv"), row.names = FALSE)
  
  return(df_clusters)
}
```

#### c. Summarise results

```{r def_summary_results}

summary_clusters = function(df){
  # df: dataframe containing a column named "cluster" 
  # which contains the cluster of each document

  # count nb of segments for each document
  df_summary = df %>% 
    group_by(fid) %>% #fid: unique identifier of each document
    dplyr::summarise(n_total = n())
  # count nb of segments in each cluster
  nb_segments = df %>% 
    group_by(cluster, fid) %>% 
    dplyr::summarise(n_cluster = n()) %>% 
    left_join(df_summary, by = "fid") %>% 
    mutate(p_cluster = round(n_cluster/n_total*100,3)) %>% 
    select(-"n_cluster", -"n_total") %>% 
    pivot_wider(names_from = cluster, names_prefix = "cl_", values_from = p_cluster) %>% 
    mutate_all(~replace(., is.na(.), 0)) # replace NA values with 0 
    return(nb_segments)
}
```

### 3. Run functions

```{r run_functions}
# read datasets
data_city_river = read.csv("1_input/data_city_river.csv")
df = read.csv("2_get_data/english/df_tokenized_snippets.csv") %>% 
  # remove the tokenized snippets which include locations
  # through trial and error we found better results without including the locations
  subset(select = -12) %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% # rename column
  dplyr::rename(seg_id = snippet_id) %>% # rename doc id 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() %>% 
  subset(!is.na(tokenized)) # remove empty values

# read corpus
corpus = read_corpus(df, text_field = "tokenized", docid_field = "seg_id")

# calculate the document-feature matrix (DFM)
dfmatrix = calc_dtm(corpus)
  # remove feature which appear in less than 10 documents (i.e. in 10 snippets)
  # remove feature which appear less than 200 times 
  # these levels have been found through trial and error 

# apply the DHC
# through trial and error, we find that the best k number in this case is 18
clusters_18 = run_dhc(df, dfmatrix, 18, 500, corpus)
# and 6 if we want less details
clusters_6 = run_dhc(df, dfmatrix, 6, 500, corpus)

# summarise results for k = 6
summary_6 = summary_clusters(clusters_6) %>% 
  separate(fid, c("fid", "supp_fid"), sep = "_") #%>% 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% 
  ungroup() %>% 
  group_by(fid) %>% 
  dplyr::mutate(cl1 = sum(as.numeric(cl_1))) %>% 
  dplyr::mutate(cl2 = sum(as.numeric(cl_2))) %>%
  dplyr::mutate(cl3 = sum(as.numeric(cl_3))) %>%
  dplyr::mutate(cl4 = sum(as.numeric(cl_4))) %>%
  dplyr::mutate(cl5 = sum(as.numeric(cl_5))) %>%
  dplyr::mutate(cl6 = sum(as.numeric(cl_6))) %>%
  dplyr::mutate(clNA = sum(as.numeric(cl_NA))) %>% 
  select(-c("supp_fid", "cl_1", "cl_2", "cl_3", "cl_4", "cl_5", "cl_6", "cl_NA")) %>% 
  unique() %>% 
  mutate(cl1 = cl1/n) %>% 
  mutate(cl2 = cl2/n) %>% 
  mutate(cl3 = cl3/n) %>% 
  mutate(cl4 = cl4/n) %>% 
  mutate(cl5 = cl5/n) %>% 
  mutate(cl6 = cl6/n) %>% 
  mutate(clNA = clNA/n) %>% 
  select(-"n") %>% 
  mutate(fid = as.numeric(fid))

# keep nb of snippets per city
nb_snippets = clusters_6[,c(1,13)] %>% 
  unique() %>% 
  mutate(fid = as.numeric(fid))

dataset = read.csv("4_clustering/selection_50_cities/clustering_300.csv") # %>%
  left_join(summary_6, by = "fid") %>% 
  left_join(nb_snippets, by = "fid") %>% 
  write.csv("4_clustering/selection_50_cities/results_clustering6.csv", row.names = FALSE)

# summarise results for k = 18
summary_18 = summary_clusters(clusters_18)
data_city_river = read.csv("1_input/data_city_river.csv") %>% 
  left_join(summary_18, by = "fid")
# and save them
write.csv(data_city_river, "4_clustering/18_clusters_371_cities.csv")
```
