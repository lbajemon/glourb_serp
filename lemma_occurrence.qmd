---
title: "Lemma occurrence calculation"
format: html
editor: visual
author: "Liolia Bajemon & Lise Vaudor"
output: rmdformats::<material>
---

```{r setup}
library(tidyverse)
library(httr)
library(jsonlite)
library(tidytext)
library(forcats)
library(ggplot2)
library(ggpattern)
library(mixr)
library(remotes)
library(mixr)
library(polyglotr)
```

## Description

Thanks to the ![](https://dka575ofm4ao0.cloudfront.net/pages-transactional_logos/retina/211386/logo_color_transparent_background.png){width="105" height="17"} API ([access link](https://get.valueserp.com/try-it-free/)), data relative to Google searches have been gathered (see this first [document](https://github.com/lbajemon/glourb_serp)) for approximately 300 combinations of agglomerations and rivers. This document aims to conduct a first analysis on the resulting data, by calculating the number of occurrences of each lemma.

```{r retrieve_data}
data_city_river = read.csv("input_data/data_city_river.csv")
```

## Clean data

The aim is to get the number of lemmas occurrence for each of our combinations. We will use an English lexicon, which indicates the lemma for each word (e.g. "is" becomes "be" and "rivers" becomes "river"):

```{r get_lexicon}
lexen = get_lexicon("en")
stop_words = tidytext::stop_words 

### get locations
# country names
country_names = data_city_river[,5] %>% #keep only the country names
  unique() %>%
  strsplit(split = " ") %>% # split the name of the country if necessary (e.g. "Republic of the Congo" gives "Republic" "of "the" "Congo")
  unlist() %>% 
  as_tibble() %>% # convert to tibble
  set_names("word") %>% 
  mutate(word = str_replace_all(word, "\\(\\)", "")) %>% # remove special char
  mutate(word = str_replace_all(word, ",", "")) %>% 
  mutate(word = str_replace_all(word, "\\(", "")) %>% 
  mutate(word = str_replace_all(word, "\\)", "")) %>% 
  mutate(word = str_replace_all(word, "[.]", "")) %>% 
  mutate(word = tolower(word)) %>% 
  # remove a few words
  subset(word != c("the", "republic", "of", "and", "autonomous", "south", "north", "central")) %>% 
  # add a few country names which have been split
  rbind("vietnam") %>% 
  rbind("ivory") %>% 
  rbind("ivoire") %>% 
  mutate(type = "location")

# city names 
global_cities = read.csv("input_data/global_cities.csv", sep = ";") 
global_cities = global_cities[,2:3] %>% # keep only the english name and name in ASCII characters
  pivot_longer(cols = everything(),
               names_to = "type",
               values_to = "word") # put everything in one column
global_cities = global_cities[,2] %>% # only keep the words column
  mutate(word = str_split(word, pattern = " ")) %>% 
  unlist() %>% 
  as.data.frame() %>%   
  mutate(type = "location") # add type 
row.names(global_cities) = NULL # rename rows 
colnames(global_cities) = c("word", "type") # rename columns

# get list of 1st level administrative division in each country
global_admin = read.csv("input_data/global_admin.csv", sep = ",") %>% 
  mutate(name = str_split(name, pattern = " ")) %>% 
  unlist() %>% 
  as.data.frame() %>% 
  mutate(type = "location")
row.names(global_admin) = NULL # rename rows
colnames(global_admin) = c("word", "type") # rename columns

# now merge all the location names 
locations = global_cities %>% # cities
  rbind(global_admin) %>% # 1st level division
  rbind(country_names) %>% # countries
  mutate(word = str_to_lower(word)) %>% # to lower case
  unique() # keep unique names
```

#### 1. In English

First let's deal with the data resulting from an English research query:

```{r def_get_words}
get_words = function(fid, cityname, rivername){
  
  ## First, get dataframe with results
  df = read.csv(glue::glue("collected_data/english/scrap_tokens/tokens_{fid}.csv")) 
  
  # get all forms of rivername and cityname to remove them from the snippets
  # for example with "N'Djaména" 
  cityname_r1 = tolower(stringi::stri_trans_general(cityname, "latin-ascii")) # n'djamena
  cityname_r2 = tolower(gsub('[[:punct:] ]+', ' ', cityname)) # n djaména
  cityname_r3 = tolower(gsub('[[:punct:] ]+', ' ', stringi::stri_trans_general(cityname, "latin-ascii"))) # n djamena
  cityname_r4 = tolower(gsub('[[:punct:] ]+', '', cityname)) # ndjaména
  cityname_r5 = tolower(gsub('[[:punct:] ]+', '', stringi::stri_trans_general(cityname, "latin-ascii"))) # ndjamena
  rivername_r1 = tolower(stringi::stri_trans_general(rivername, "latin-ascii")) 
  rivername_r2 = tolower(gsub('[[:punct:] ]+', ' ', rivername)) 
  rivername_r3 = tolower(gsub('[[:punct:] ]+', ' ', stringi::stri_trans_general(rivername, "latin-ascii"))) 
  rivername_r4 = tolower(gsub('[[:punct:] ]+', '', rivername)) 
  rivername_r5 = tolower(gsub('[[:punct:] ]+', '', stringi::stri_trans_general(rivername, "latin-ascii"))) 
  # combine everything in a df
  names_r = data.frame(matrix(ncol = 1, nrow = 0))
  names_r = names_r %>% 
    rbind(cityname_r1, cityname_r2, cityname_r3, cityname_r4, cityname_r5, rivername_r1, rivername_r2, rivername_r3, rivername_r4, rivername_r5) 
  colnames(names_r) = "word"
  names_r = names_r %>% 
    mutate(word = strsplit(word, " ")) %>% 
    unnest(word)  
  names_r = names_r %>% 
    mutate(word = strsplit(word, split = ";", fixed = TRUE)) %>% 
    unnest(word) %>% 
    unique()

  ## Now, we consider all words
  df_words = df %>% 
    unnest_tokens(word, text_en, to_lower = TRUE) %>% # everything to lowercase
    # get lemma of each word
    left_join(lexen, by = "word") %>% 
    group_by(lemma) %>% 
    dplyr::summarise(n = n()) %>% 
    arrange(desc(n)) %>% 
    select(word = lemma, n) %>% 
    na.omit() %>% 
    # remove the words corresponding to days or months
    filter(!(word %in% c("jan","feb","mar","apr","may","jun",
                     "jul","aug","sep","oct","nov","dec",
                     "january","february","march","april",
                     "june","july","august","september",
                     "october","november","december",
                     "monday","tuesday","wednesday",
                     "thursday","friday","saturday","sunday"))) %>% 
    # remove stop words
    anti_join(stop_words, by = "word") %>% 
    # remove river and city names
    anti_join(names_r, by = "word") %>% 
    arrange(desc(n)) %>% 
    # remove solitary letters
    mutate(word = gsub("\\b[a-zA-Z]\\b", "", word)) %>% 
    # remove empty rows
    subset(word != "") %>% 
    na.omit()
  
  # save the results  
  print(cityname)
  readr::write_csv(df_words,
                   glue::glue("analysis/occurrence/english/lemma_occ/words_{fid}.csv"))
  return()
}
```

Now let's run the function:

```{r, run_get_words}
data_city_river %>% 
  mutate(data = purrr::pmap(list(fid = fid,
                                 cityname = city_en,
                                rivername = river_en),
                            get_words))
```

#### 2. In the local language(s)

It's basically the same process as for the query in English:

```{r def_get_words_hl}
get_words_hl = function(fid, cityname, rivername, hl){
  
  if(file.exists(glue::glue("collected_data/hl/scrap_tokens/tokens_{fid}_{hl}.csv"))){
  ## First, get dataframe with results
  df = read.csv(glue::glue("collected_data/hl/scrap_tokens/tokens_{fid}_{hl}.csv")) 
  
  # get all forms of rivername and cityname to remove them from the snippets
  # for example with "N'Djaména" 
  cityname_r1 = tolower(stringi::stri_trans_general(cityname, "latin-ascii")) # n'djamena
  cityname_r2 = tolower(gsub('[[:punct:] ]+', ' ', cityname)) # n djaména
  cityname_r3 = tolower(gsub('[[:punct:] ]+', ' ', stringi::stri_trans_general(cityname, "latin-ascii"))) # n djamena
  cityname_r4 = tolower(gsub('[[:punct:] ]+', '', cityname)) # ndjaména
  cityname_r5 = tolower(gsub('[[:punct:] ]+', '', stringi::stri_trans_general(cityname, "latin-ascii"))) # ndjamena
  rivername_r1 = tolower(stringi::stri_trans_general(rivername, "latin-ascii")) 
  rivername_r2 = tolower(gsub('[[:punct:] ]+', ' ', rivername)) 
  rivername_r3 = tolower(gsub('[[:punct:] ]+', ' ', stringi::stri_trans_general(rivername, "latin-ascii"))) 
  rivername_r4 = tolower(gsub('[[:punct:] ]+', '', rivername)) 
  rivername_r5 = tolower(gsub('[[:punct:] ]+', '', stringi::stri_trans_general(rivername, "latin-ascii"))) 
  # combine everything in a df
  names_r = data.frame(matrix(ncol = 1, nrow = 0))
  names_r = names_r %>% 
    rbind(cityname_r1, cityname_r2, cityname_r3, cityname_r4, cityname_r5, rivername_r1, rivername_r2, rivername_r3, rivername_r4, rivername_r5) 
  colnames(names_r) = "word"
  names_r = names_r %>% 
    mutate(word = strsplit(word, " ")) %>% 
    unnest(word)  
  names_r = names_r %>% 
    mutate(word = strsplit(word, split = ";", fixed = TRUE)) %>% 
    unnest(word) %>% 
    unique()

  ## Now, we consider all words
  df_words = df %>% 
    unnest_tokens(word, text_en, to_lower = TRUE) %>% # everything to lowercase
    # get lemma of each word
    left_join(lexen, by = "word") %>% 
    group_by(lemma) %>% 
    dplyr::summarise(n = n()) %>% 
    arrange(desc(n)) %>% 
    select(word = lemma, n) %>% 
    na.omit() %>% 
    # remove the words corresponding to days or months
    filter(!(word %in% c("jan","feb","mar","apr","may","jun",
                     "jul","aug","sep","oct","nov","dec",
                     "january","february","march","april",
                     "june","july","august","september",
                     "october","november","december",
                     "monday","tuesday","wednesday",
                     "thursday","friday","saturday","sunday"))) %>% 
    # remove stop words
    anti_join(stop_words, by = "word") %>% 
    # remove river and city names
    anti_join(names_r, by = "word") %>% 
    arrange(desc(n)) %>% 
    # remove solitary letters
    mutate(word = gsub("\\b[a-zA-Z]\\b", "", word)) %>% 
    # remove empty rows
    subset(word != "") %>% 
    na.omit()
  
  print(fid)
  # save the results  
  readr::write_csv(df_words,
                   glue::glue("analysis/occurrence/hl/lemma_occ/word_{fid}_{hl}.csv"))
  return()
  }
  else {
    print("no data")
    }
}
```

```{r run_get_words_hl}

#### define in which language we will run the functions
# the languages used in a city are in the 9th to 14th columns

for(i in 1:6) {
  n_hl = 7 + i 
  data_subset = filter(data_city_river, ((data_city_river[, n_hl] != "no data")))
  data = select(.data = data_subset, c(1:7, n_hl, 14, 21))
  colnames(data) = c("fid", "latitude", "longitude", "urban_aggl", "country_en", "country_fr", "gl", "hl", "cityname", "rivername")

data %>% 
  mutate(data = purrr::pmap(list(fid = fid,
                                 cityname = cityname,
                                 rivername = rivername,
                                 hl = hl),
                            get_words_hl))
  }
```

### Plot results

Finally let's display our results for all the combinations:

#### 1. In English

```{r show_results}
show_results = function(fid, ville, riviere){
  
  # read the data
  freq = read.csv(file = glue::glue("analysis/occurrence/english/lemma_occ/words_{fid}.csv"))

  # select the data we want to display
  freq_cut = freq[freq$n > 1,] # only show the words which appear at least twice
  freq_cut = freq_cut[1:30,] # only display 30 words so it's more readable
 
  # make the plot
  myplot = freq_cut %>% # our data
    mutate(word = fct_reorder(word, n)) %>% # rearrange in decreasing order
    ggplot(mapping = aes(x = word,
                         y = n)) + 
    geom_col(fill = "#b2df8a", position = "stack") +
    coord_flip() + # flip x and y coordinates 
    labs(x = "lemme",
         y = "nombre d'occurrences",
         title = paste0(ville, " et ", riviere)) +
    theme_bw(base_family = "CenturySch")

  # save the plot
  ggsave(filename = paste0("plot_", fid, ".png"), plot = myplot, path = "analysis/occurrence/english/plots/", width = 20, height = 12, units = "cm")
  return()
} 
```

```{r run_show_results}
data_city_river %>% 
  mutate(data = purrr::pmap(list(fid = fid,
                                 ville = ville,
                                 riviere = riviere),
                            show_results))
```

#### 2. In the local language(s)

```{r show_results_hl}
show_results_hl = function(fid, ville, riviere, hl){
  
  # get the table containing all the languages
  data_language = read.csv("input_data/languages_code.csv", sep = ";")
  # and select the name of the language we are interested in 
  language = data_language[data_language["code"] == hl, 1]
  
  # if the file already exists, the function is not executed
    if(file.exists(glue::glue("analysis/occurrence/hl/lemma_occ/word_{fid}_{hl}.csv"))) {
    
  # read the data
  freq = read.csv(file = glue::glue("analysis/occurrence/hl/lemma_occ/word_{fid}_{hl}.csv"))
  
  # select the data we want to display
  freq_cut = freq[freq$n > 1,] # only show the words which appear at least twice
  freq_cut = freq_cut[1:30,] # only display 20 words so it's more readable
  
  # make the plot
  myplot = freq_cut %>% # our data
    mutate(word = fct_reorder(word, n)) %>% # rearrange in decreasing order
    ggplot(mapping = aes(x = word, # create the plot
                         y = n)) + 
    geom_col(fill = "#fb9a99", position = "stack") +
    coord_flip() + # flip x and y coordinates 
    labs(title = paste0(ville, " et ", riviere), 
         subtitle = paste("Requête en", language),
         x = "lemme",
         y = "nombre d'occurrences"
    ) + 
    theme_bw(base_family = "CenturySch")

  ggsave(filename = paste0("plot_",fid,"_", hl,".png"), plot = myplot, path = "analysis/occurrence/hl/plots/", width = 20, height = 12, units = "cm")

  return()
    } 
}
```

```{r run_show_results_hl}

#### define in which language we will run the functions
# the languages used in a city are in the 9th to 14th columns

for(i in 1:6) {
 n_hl = 7 + i 
  data_subset = filter(data_city_river, ((data_city_river[, n_hl] != "no data")))
  data = select(.data = data_subset, c(1:7, n_hl, 34, 35))
  colnames(data) = c("fid", "urban_aggl", "latitude", "longitude",  "country_en", "country_fr", "gl", "hl", "ville", "riviere")

#### RESULTS
data %>% 
  mutate(data = purrr::pmap(list(fid = fid,
                                 ville = ville,
                                 riviere = riviere,
                                 hl = hl),
                            show_results_hl))
  }
```

### Example: Lyon

```{r read_lyon_data}
df_tokens_en = read.csv("collected_data/english/df_tokens.csv")
df_lyon_en = df_tokens_en %>% 
  filter(df_tokens_en$urban_aggl == "Lyon")
df_tokens_hl = read.csv("collected_data/hl/df_tokens_all_hl.csv")
df_lyon_hl = df_tokens_hl %>% 
  filter(df_tokens_hl$urban_aggl == "Lyon")
```

```{r}
  df = read.csv(glue::glue("collected_data/english/scrap_tokens/tokens_{fid}.csv")) 

  df_count = df %>% 
    unnest_tokens(word, tokenized_noloc, to_lower = TRUE) %>% # everything to lowercase
    group_by(word) %>% 
    dplyr::summarise(n = n()) %>% 
    arrange(desc(n)) 
  
  df_count[1:20,] %>% # our data
      ggplot(mapping = aes(x = fct_reorder(word, n),
                         y = n)) + 
    geom_col(fill = "#8DD3C7") +
    coord_flip() + # flip x and y coordinates 
    labs(title = "Sabarmati river - Ahmedabad",
         x = "lemme",
         y = "nombre d'occurrences"
    ) + 
    theme_classic() + 
    theme(plot.subtitle = element_text(face = "italic"))
  
```

co-occ

```{r}
library(quanteda)

corpus = corpus(df$tokenized_text, docnames = df$position)
tokens = corpus %>% 
  tokens()

tokClmot = tokens_keep(tokens, "gravel", window = 5)
tokClmot = fcm(tokClmot)
featCl = names(topfeatures(tokClmot, 30))
net = fcm_select(tokClmot, pattern = featCl)
textplot_network(net)
```
