---
title: "Search Engine Pages Results: Clustering and co-occurrences"
format: html
editor: visual
author: "Liolia Bajemon"
---

### Description

The aim of this document is to do a clusterisation on a corpus.

Our corpus consists of snippets (or "rich snippets", i.e. the first lines extracted from a search engine pages results - SERP) extracted from multiple queries on Google. 319 queries of the type "water AND {cityname}" were ran, in English and in each local language of the given city.

This method uses the [rainette package](https://juba.github.io/rainette/), which is based on a variant of the Reinert textual clustering method. This classification method is used for short text segments, which applies to our case, the snippets being approximately thirty words long.

### Set-up

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(magrittr)
library(tidyr)
library(plyr)
library(purrr)
library(stringr)
library(mixr)
library(tokenizers)
library(lexicon)
library(dplyr)
library(quanteda)
library(rainette)
library(rlist)
library(tmaptools)
library(ggplot2)
library(shiny)
library(FactoMineR)
library(quanteda.textplots)
set.seed(850) # set seed to get reproductible results

data_city_river = read.csv("input_data/data_city_river.csv", sep = ",") %>% 
    distinct(urban_aggl, .keep_all = TRUE)
```

```{r def_palette}
# def palette
mypalette = get_brewer_pal("Set3", 9) 
mypalette[2] = "#e6ab02" # replace pale yellow 
mypalette[9] = "#bc80bd" # replace gray
big_palette = c(mypalette, mypalette)
```

### 1. Read corpus

First, let's import our dataset and convert it to a corpus using the quanteda package:

```{r def_read_corpus}
read_corpus = function(df, text_field, docid_field){
  my_corpus = quanteda::corpus(df, docid_field, text_field)
  }
```

```{r def_tokens}
read_tokens = function(corpus){
  tok_serp = tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, split_hyphens = TRUE) # tokenization and removal of diverse items
  }
```

The tokenized snippets are already quite short, so we can skip the step of splitting the text in segments (using `split_segments`) and move on to the calculation of the document-feature matrix (dfm). The dfm is a mathematical matrix describing for each term, its frequency in each document. In rows are the documents (i.e. one row per snippet) and in columns are the terms.

### 2. Define functions for clustering

#### a. Calculate the DFM

```{r def_calc_dtm}
calc_dtm = function(tokens){
  # calculate the DFM
  dfmatrix = dfm(tokens, remove_padding = TRUE) # remove NA or empty values
  # remove feature which appear in a given number of documents 
  # and at least a given number of times 
  dfmatrix <- dfm_trim(dfmatrix, min_docfreq = 10, min_termfreq = 200)
}
```

#### b. Apply clustering

```{r dhc, warning = FALSE, message = FALSE}
run_dhc = function(df, dfmatrix, n_class, min_members, corpus, mylabels, mycolors, path, measure, filename){

  # df: original dataset
  # dfmatrix: the document feature matrix calculated beforehand
  # n_class: number of clusters
  # min_members: do not split the cluster again if it contains less than 500 members
  # corpus: corpus imported beforehand
  
  k = n_class
  min_split_members = min_members

  dhc = rainette(dfmatrix, k, min_split_members = min_split_members, min_segment_size = 0)

# plot
  plot = rainette_plot(
    dhc, dfmatrix, k,
    n_terms = 20,
    free_scales = TRUE,
    measure = measure,
    show_negative = FALSE,
    text_size = 12,
    cluster_label = mylabels, 
    colors = mycolors
  )
  ggsave(path, plot, width = 500, height = 250, units = "mm", limitsize = FALSE)

  # save the results (i.e. most frequent terms for each cluster)
  groups = cutree_rainette(dhc, k = n_class)
  groups_stats = rainette_stats(groups, dfmatrix)
  list.save(groups_stats, glue::glue("analysis/floods/clustering/resultats_statistiques_{filename}.rds"))
  
  # for each element of the corpus (i.e. each snippet)
  # write its class according to the clustering
  corpus$cluster = cutree_rainette(dhc, k)
  # create a df indicating the cluster for each doc 
  docvar = docvars(corpus) %>% 
    mutate(seg_id = paste0(fid, "_", position)) %>% 
    # seg_id: unique identifier of each segment 
    subset(select = -c(1:(ncol(docvars(corpus))-1)))
  # join with our original dataset
  df_clusters = df %>% 
    left_join(docvar, by = "seg_id", keep = FALSE)
# and save the result
  write.csv(df_clusters, glue::glue("analysis/floods/clustering/classified_snippets_{filename}.csv"), row.names = FALSE)
  
  return(df_clusters)
}
```

#### c. Summarise results

```{r def_summary_results}

summary_clusters = function(df){
  # df: dataframe containing a column named "cluster" 
  # which contains the cluster of each document

  # count nb of segments for each document
  df_summary = df %>% 
    group_by(fid) %>% #fid: unique identifier of each document
    dplyr::summarise(n_total = n())
  # count nb of segments in each cluster
  nb_segments = df %>% 
    group_by(cluster, fid) %>% 
    dplyr::summarise(n_cluster = n()) %>% 
    left_join(df_summary, by = "fid") %>% 
    mutate(p_cluster = round(n_cluster/n_total*100,3)) %>% 
    select(-"n_cluster", -"n_total") %>% 
    pivot_wider(names_from = cluster, names_prefix = "cl_", values_from = p_cluster) %>% 
    mutate_all(~replace(., is.na(.), 0)) # replace NA values with 0 
    return(nb_segments)
}
```

### 3. Run functions

#### a. For results in English

First, let's read our data, which corresponds to all the "tokenized snippets" for the queries in English (see [here](https://github.com/lbajemon/glourb_serp/blob/main/lemmatisation.qmd)). There are two different tokenized snippets for each element: one including locations (e.g. "France, "Lyon", *etc.*) and one excluding them. Through trial and error, we find that in this case it is better to exclude them from the corpus. For the calculation of the document-feature matrix, we remove features which appear in less than 10 documents and which appear less than 200 times.

```{r read_data_en}

# read datasets

df = read.csv("collected_data/floods/english/df_tokenized_snippets.csv") %>% 
  dplyr::rename(seg_id = snippet_id) %>% # rename doc id 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() %>% 
  subset(!is.na(tokenized)) # remove empty values

# read corpus
corpus = read_corpus(df, text_field = "tokenized", docid_field = "seg_id")
tokens = read_tokens(corpus)

# calculate the document-feature matrix (DFM)
dfmatrix = calc_dtm(tokens)

# we suggest to try multiple number of clusters and minimum split members 
# and choose what adapts best to your data

dhc = rainette(dfmatrix, k = 10, min_split_members = 500)
rainette_explor(dhc, dfmatrix, corpus)
```

We will apply one clusterisation with 9 clusters and we will keep the tokens of the type "location".

```{r 9_clusters_en}
mylabels = c("Niveau de l'eau", "Institutions, organismes", "Gestion et connaissance\ndu risque", "Electricité, éclairage\net énergie", "Météo et\nconditions climatiques", "Impacts et\ndégâts matériels", "Causes de l'inondation,\ndescription", "Victimes et gestion\nde la crise", "Internet")
clusters_9 = run_dhc(df, dfmatrix, 9, 500, tokens, mylabels, big_palette, path = "analysis/floods/clustering/clustering_en_k9_loc.png", measure = "frequency", filename = "9_en_loc")
```

#### b. For results in all local languages

Now we will apply a clustering to all the results found from the queries in all local languages (up to 6 per city).

```{r read_data_hl}
df_loc = read.csv("collected_data/floods/hl/df_tokenized_all.csv") %>% 
  dplyr::rename(seg_id = snippet_id) %>% # rename doc id 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() %>% 
  subset(!is.na(tokenized)) # remove empty values
df_noloc = read.csv("collected_data/floods/hl/df_tokenized_all.csv") %>%
  dplyr::rename(seg_id = snippet_id) %>% # rename doc id 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() %>% 
  subset(!is.na(tokenized_noloc)) # remove empty values

corpus_loc = read_corpus(df_loc, text_field = "tokenized", docid_field = "seg_id")
tokens_loc = read_tokens(corpus_loc)
corpus_noloc = read_corpus(df_noloc, text_field = "tokenized_noloc", docid_field = "seg_id")
tokens_noloc = read_tokens(corpus_noloc)

dfmatrix_loc = calc_dtm(tokens_loc)
dfmatrix_noloc = calc_dtm(tokens_noloc)

# try multiple numbers of clusters and choose the best one
dhc_loc = rainette(dfmatrix_loc, k = 20, min_split_members = 500)
rainette_explor(dhc_loc, dfmatrix_loc, corpus_loc)
dhc_noloc = rainette(dfmatrix_noloc, k = 20, min_split_members = 500)
rainette_explor(dhc_noloc, dfmatrix_noloc, corpus_noloc)
```

The optimal numbers of clusters are 9 (including locations) and 9 (excluding locations)

##### b.1. 9 clusters: with locations

```{r 9_clusters_all_hl}
dhc = rainette(dfmatrix_loc, 9, min_split_members = 500, min_segment_size = 0)

# plot
plot = rainette_plot(
  dhc, 
  dfmatrix_loc, 
  9,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Outlier", "Electricité,\nentreprises", "Niveau de l'eau,\nhydrographie", "Gouvernance,\ngestion de la crise,\nconnaissances", "Conditions météorologiques,\ncauses de l'inondation", "Victimes, dégâts", "Internet et\ninformations","Politique" ,"Outlier`\n(réseaux sociaux)"),
  colors = big_palette
  )
ggsave("analysis/floods/clustering/clustering_all_hl_k9_loc.png", plot, width = 500, height = 250, units = "mm", limitsize = FALSE)

# save the results (i.e. most frequent terms for each cluster)
groups_loc = cutree_rainette(dhc, k = 9)
groups_stats = rainette_stats(groups_loc, dfmatrix_loc)
list.save(groups_stats, glue::glue("analysis/floods/clustering/resultats_statistiques_9_all_hl_loc.rds"))
  
  # for each element of the corpus (i.e. each snippet)
  # write its class according to the clustering
  corpus_loc$cluster = cutree_rainette(dhc, 9)
  # create a df indicating the cluster for each doc 
  docvar = docvars(corpus_loc) %>% 
    mutate(seg_id = paste0(fid, "_", hl, "_", position)) %>% 
    # seg_id: unique identifier of each segment 
    subset(select = -c(1:(ncol(docvars(corpus_loc))-1)))
  # join with our original dataset
  df_clusters = df_loc %>% 
    left_join(docvar, by = "seg_id", keep = FALSE)
# and save the result
write.csv(df_clusters, glue::glue("analysis/floods/clustering/classified_snippets_9_all_hl_loc.csv"), row.names = FALSE)

# summarise results
summary_9 = summary_clusters(df_clusters)
data_city_river = read.csv("input_data/data_city_river.csv") %>% 
  left_join(summary_9, by = "fid")
# and save them
write.csv(data_city_river, "analysis/floods/clustering/summary_9_all_hl_loc.csv", row.names = FALSE)
```

##### b.2. 12 clusters: without locations

```{r 12_clusters_hl}
dhc = rainette(dfmatrix_noloc, 12, min_split_members = 500, min_segment_size = 0)

# plot
plot = rainette_plot(
  dhc, 
  dfmatrix_noloc, 
  12,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Outlier", "Conditions météorologiques,\ncauses de l'inondation", "Victimes", "Hydrographie", "Internet et\ninformations", "Gouvernance, gestion\nde la crise", "Transports", "Connaissances\net aménagement", "Dégâts matériels", "Élévation du niveau\nde la mer", "Électricité et\nentreprises", "Outlier (vidéos/\nréseaux sociaux)"),
  colors = big_palette
  )
ggsave("analysis/floods/clustering/clustering_all_hl_k12_noloc.png", plot, width = 500, height = 250, units = "mm", limitsize = FALSE)

# save the results (i.e. most frequent terms for each cluster)
groups = cutree_rainette(dhc, k = 12)
groups_stats = rainette_stats(groups, dfmatrix_noloc)
list.save(groups_stats, glue::glue("analysis/floods/clustering/resultats_statistiques_12_all_hl_noloc.rds"))
  
  # for each element of the corpus (i.e. each snippet)
  # write its class according to the clustering
  corpus_noloc$cluster = cutree_rainette(dhc, 12)
  # create a df indicating the cluster for each doc 
  docvar = docvars(corpus_noloc) %>% 
    mutate(seg_id = paste0(fid, "_", hl, "_", position)) %>% 
    # seg_id: unique identifier of each segment 
    subset(select = -c(1:(ncol(docvars(corpus_noloc))-1)))
  # join with our original dataset
  df_clusters = df_noloc %>% 
    left_join(docvar, by = "seg_id", keep = FALSE)
# and save the result
write.csv(df_clusters, glue::glue("analysis/floods/clustering/classified_snippets_12_all_hl_noloc.csv"), row.names = FALSE)

# summarise results
summary_12 = summary_clusters(df_clusters)
data_city_river = read.csv("input_data/data_city_river.csv") %>% 
  left_join(summary_12, by = "fid")
# and save them
write.csv(data_city_river, "analysis/floods/clustering/summary_12_all_hl_noloc.csv", row.names = FALSE)
```
