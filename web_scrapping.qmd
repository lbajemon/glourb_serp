---
title: "Web scrapping with rvest"
author: "Liolia Bajemon"
format: html
editor: visual
---

### Description

The aim of this document is to extract content from given websites â€“ in other words, *web scraping*. The links of the websites were retrieved beforehand using SERP (see [here](https://github.com/lbajemon/glourb_serp/blob/main/get_data_river.qmd)). Multiple packages will be used, including one from Python. The cleaning process of the HTML content was inspired by B. Rudis ([2017](https://rud.is/b/2017/08/24/reticulating-readability/)).

### Set-up

```{r packages, warning = FALSE, message = FALSE, comment = FALSE}
library(httr)
library(reticulate)
library(xml2)
library(tidyverse)
library(rvest)
library(RSelenium)
library(R.utils)
library(polyglotr)

# install python if necessary: 
# install_python(version = "3.10:latest")
# install readability
reticulate::py_install(c("readability-lxml", "lxml_html_clean"))
# import the readability library from python
readability = reticulate::import("readability") # pip install readability-lxml
```

### Define functions for web scraping

First, let's define the function which will scrape the websites.

```{r def_scrape_page}
scrap_page = function(url){
  print(url)
  
  # Scrap URL
  result = tryCatch({
    withTimeout({
      Sys.sleep(sample(1:3, 1))
      # define an user-agent to prevent being blocked from some websites
      res = GET(url = url, config(followlocation = 10), add_headers("user-agent" = "Mozilla/5.0"), set_cookies("cookies" = "value"))

      # check if the response is valid
      if (is.null(res) || !inherits(res, "response") || httr::status_code(res) != 200) {
        cat("Non-200 status code or NULL response for: ", url, "\n")
        return(NA_character_)
      } else {
        print("valid status code")
        }
  
      # extract HTML content 
      html_content = tryCatch({
        httr::set_config(httr::user_agent("Mozilla/5.0"))
        httr::content(res, as = "text", encoding = "UTF-8")
      }, error = function(e) {
        cat("Error when extracting content for: ", url, ":", e$message, "\n")
        return(NULL)
      })

      # check if HTML content is valid
      if(is.null(html_content) || is.na(html_content) || html_content == "" || !grepl("<html", html_content, ignore.case = TRUE)) {
        cat("Invalid HTML content or non-HTML content for: ", url, "\n")
        return(NA_character_)
      } else {
      print("valid HTML content")
        }
  
      # clean HTML using rvest (read_html)
      html_clean = tryCatch({
        read_html(html_content)
      }, error = function(e) {
        cat("Error when analysing the HTML file for: ", url, ":", e$message, "\n")
        return(NULL)
      })

      if (is.null(html_clean)) {
        cat("Failed to parse HTML for: ", url, "\n")
        return(NA_character_)
      } else {
        print("HTML was successfully cleaned")
        }
  
      # Convert HTML to text
      html_clean_text = as.character(html_clean)
   
      summary_text = tryCatch({
        # use readability to keep only text
        doc = readability$Document(html_clean_text)
        # get simplified content 
        summary_html = doc$summary()
        # read XML content and extract text
        read_html(summary_html) %>%
          html_text()
      }, error = function(e) {
        cat("error when extracting text from HTML summary for: ", url, ":", e$message, "\n")
        return(NULL)
      })
    
        if(is.null(summary_text)){
          cat("The HTML summary cannot be read for:", url, "\n")
          return(NA_character_)
        } else {
          print("Text was extracted successfully")
          }
  
    return(summary_text)
      # if extracting the content takes too long, return NA 
  }, timeout = 50, onTimeout = "error")
}, TimeoutException = function(ex){
  cat("Timeout exceeded for:", url, "\n")
  return(NA_character_)
}, error = function(e){
  cat("error occurred for:", url, ":", e$message, "\n")
  return(NA_character_)
})
  return(result)
}
```

After retrieving the content from the websites, the data needs to be cleaned and organized in a dataframe.

```{r def_process_data}

process_data_en = function(fid){
  print(fid)
  if(!file.exists(glue::glue("collected_data/english/scrap_data/scrap_{fid}.csv"))){
    # read data
    df = read.csv(glue::glue("collected_data/english/clean_data/clean_{fid}.csv")) %>% 
    # create a new empty column
    cbind(text = NA_character_)

    df = df %>% 
      # using the link of the website, scrap its content
      mutate(text = purrr::map(link, scrap_page)) %>% 
      # transform the list to strings
      unnest(text) %>% 
      # replace \n which correspond to a carriage return
      mutate(text = str_replace_all(text, "\n", " ")) %>% 
      # save the results
      write.csv(glue::glue("collected_data/english/scrap_data/scrap_{fid}.csv"), row.names = FALSE)
  }
}

process_data_hl = function(fid, hl){
  print(fid)
  if(!file.exists(glue::glue("collected_data/hl/scrap_data/scrap_{fid}_{hl}.csv"))){
    if(file.exists(glue::glue("collected_data/hl/clean_data/clean_{fid}_{hl}.csv"))){
    # read data
    df = read.csv(glue::glue("collected_data/hl/clean_data/clean_{fid}_{hl}.csv")) %>% 
    # create a new empty column
    cbind(text = NA_character_)

    df = df %>% 
      # using the link of the website, scrap its content
      mutate(text = purrr::map(link, scrap_page)) %>% 
      # transform the list to strings
      unnest(text) %>% 
      # replace \n which correspond to a carriage return
      mutate(text = str_replace_all(text, "\n", " ")) %>% 
      # save the results
      write.csv(glue::glue("collected_data/hl/scrap_data/scrap_{fid}_{hl}.csv"), row.names = FALSE)
    }
  }
  else print("No Data")
}
```

### Run functions

Finally, let's run the functions for each of our cities:

```{r run_functions}

data_city_river = read.csv("input_data/data_city_river.csv", sep = ",")

# in english
data_city_river %>% 
  mutate(data = purrr::map(fid, process_data_en))

# in local languages 
for(i in 1:6) {
  n_hl = 7 + i 
  data_subset = filter(data_city_river, ((data_city_river[, n_hl] != "no data") & ((data_city_river[, n_hl] != "en") | data_city_river[, 7] != "us")))
  data = select(.data = data_subset, c(1, n_hl))
  colnames(data) = c("fid", "hl")
  
data %>% 
  mutate(data = purrr::pmap(list(fid = fid,
                                 hl = hl),
                            process_data_hl))
}
```

### Translate scrapped data

Now we will translate all the scrapped data so that everything is in English. First, we must define a function which will translate the text only if it is not English.

```{r def_translation}
translation = function(text){
  # check if there is any text
  if(is.na(text) || text == "" || grepl("^\\s*$", text) || is.null(text)){
    return(NA_character_)
  }
  
  # shorten the text to detect its language
  short_text = substr(text, 1, 4000)
  lang = tryCatch({
    language_detect(short_text)
  }, error = function(e){
      NA
    })
  
  if(is.na(lang)){
    return(NA_character_)
    }

  # translate the text if necessary
  if(lang != "en"){
    trans_text = tryCatch({
      google_translate(text, "en", "auto")
    }, error = function(e) {
       # if an error occurs, return NA
        NA
      })
 
    # check if google_translate returned character(0). If so, return NA:
    if(length(trans_text) == 0 || is.na(trans_text)){
      return(NA)
    } else {
      return(trans_text)
    }
  } else {
    return(text)
  }
}
```

Then we define a function which deals with all of our dataframes and translates the text when necessary:

```{r def_translate_text}
translate_text = function(fid){
  print(fid)
  file = glue::glue("collected_data/english/scrap_translated/scrap_t_{fid}.csv")
  
  # if the file already exists, the function will not be run.
  if(!file.exists(file)){
    df = read.csv(glue::glue("collected_data/english/scrap_data/scrap_{fid}.csv", fileEncoding = "UTF-8"))
    
  ### CLEAN 
  ## remove the URL because they cause an error with google translate 
  df = df %>% 
    mutate(text = str_replace_all(text, "http://", " ")) %>% 
    mutate(text = str_replace_all(text, "https://", " ")) %>% 
    mutate(text = str_replace_all(text, "%", " ")) %>% 
    mutate(text = str_replace_all(text, "://", " ")) %>% 
    mutate(text = str_replace_all(text, "\t", " ")) %>% 
    mutate(text = str_replace_all(text, "\n", " ")) %>% 
    rowwise() %>% 
    # translate text when necessary
    mutate(text_en = translation(text)) %>% 
    ungroup() %>% 
    mutate(text_en = case_when(
      !is.na(text_en) ~ text_en,
      # if no text has been found, use the snippet instead
      is.na(text_en) & !is.na(trans_snippet) ~ trans_snippet,
      # if there neither text or a snippet, return NA
      TRUE ~ NA
      ))
  
  ### SAVE 
  # write the results in a new file
  readr::write_csv(df,
                   glue::glue("collected_data/english/scrap_translated/scrap_t_{fid}.csv"))
  }
}
```

Finally let's run the function to translate all of our dataframes:

```{r run_translated}
data_city_river %>% 
  mutate(data = purrr::map(fid,
                          translate_text))
```
