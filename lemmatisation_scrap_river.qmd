---
title: "Lemmatisation of the scrapped websites"
format: html
editor: visual
author: "Liolia Bajemon & Lise Vaudor"
---

### Description and set-up

The aim of this document is to lemmatise and clean our snippets.

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(tidyverse)
library(mixr) # for the lexicon
library(polyglotr) # for translation
library(tidytext)
library(plyr)
```

### Define useful datasets

Get our data set with all the river + city combinations.

```{r read_data}
data_city_river = read.csv("input_data/data_city_river.csv")

# get the tld and google domain for each country
data_google_domain = read.csv("input_data/country_code_internet.csv")
data_google_domain = subset(data_google_domain, select = c("gl", "tld", "google_domain"))
# and put it in our table
data_city_river = left_join(data_city_river, data_google_domain, by = "gl", relationship = "many-to-many")
# note: there is no google domain for Iran, Soudan, Syria and North Korea, so we put google.com instead

#  format the data for the search queries, i.e. put "OR" instead of ";" to indicate multiple cities or rivers: 
# also replace - by OR
data_city_river = data_city_river %>% 
    mutate(city_en = str_replace_all(city_en, ";", " OR ")) %>% 
    mutate(city_hl1  = str_replace_all(city_hl1, ";", " OR ")) %>% 
    mutate(city_hl2 = str_replace_all(city_hl2, ";", " OR ")) %>% 
    mutate(city_hl3 = str_replace_all(city_hl3, ";", " OR ")) %>% 
    mutate(city_hl4 = str_replace_all(city_hl4, ";", " OR ")) %>% 
    mutate(city_hl5 = str_replace_all(city_hl5, ";", " OR ")) %>% 
    mutate(city_hl6 = str_replace_all(city_hl6, ";", " OR ")) %>% 
    mutate(river_en = str_replace_all(river_en, ";", " OR ")) %>% 
    mutate(river_hl1 = str_replace_all(river_hl1, ";", " OR ")) %>% 
    mutate(river_hl2 = str_replace_all(river_hl2, ";", " OR ")) %>% 
    mutate(river_hl3 = str_replace_all(river_hl3, ";", " OR ")) %>% 
    mutate(river_hl4 = str_replace_all(river_hl4, ";", " OR ")) %>% 
    mutate(river_hl5 = str_replace_all(river_hl5, ";", " OR ")) %>% 
    mutate(river_hl6 = str_replace_all(river_hl6, ";", " OR ")) 

# in the value SERP API, the language code for chinese (simplified) is zh-cn. Let's update our data table:
data_city_river = data_city_river %>% 
  mutate(hl1 = str_replace_all(hl1, "zh", "zh-cn")) %>% 
  mutate(hl2 = str_replace_all(hl2, "zh", "zh-cn")) %>% 
  mutate(hl3 = str_replace_all(hl3, "zh", "zh-cn")) %>% 
  mutate(hl4 = str_replace_all(hl4, "zh", "zh-cn")) %>% 
  mutate(hl5 = str_replace_all(hl5, "zh", "zh-cn")) %>% 
  mutate(hl6 = str_replace_all(hl6, "zh", "zh-cn"))

lexicon_en = get_lexicon("en") %>% 
  rbind(read.csv("input_data/ignore/missing_words_lexicon_en2.csv"))
```

Now, let's define multiple tables containing words we want to remove or keep, e.g. stop words, country names, city names, etc.

In particular, we will try to define a list of locations, including city names, country names and region names. We will use a [database from Geonames](https://public.opendatasoft.com/explore/dataset/geonames-all-cities-with-a-population-1000/information/?disjunctive.cou_name_en&sort=name), which supposedly contains the name of the cities with a population \> 1000 worldwide. We will also use diverse sources to get the first-level administrative divisions by country (in particular, this [Wikipedia](https://en.wikipedia.org/wiki/List_of_first-level_administrative_divisions_by_area) page or the first-level NUTS of the EU).

```{r}
# stop words (words which are too common)
stop_words = tidytext::stop_words 

# days and months
dates = data.frame(word = c("jan","feb","mar","apr","may","jun", "jul","aug","sep","oct","nov","dec", "january","february","march","april", "june","july","august","september", "october","november","december", "monday","tuesday","wednesday", "thursday","friday","saturday","sunday")) 

# country names
country_names = data_city_river[,5] %>% # keep only the country names
  unique() %>%
  strsplit(split = " ") %>% # split the name of the country if necessary (e.g. "Republic of the Congo" gives "Republic" "of "the" "Congo")
  unlist() %>% 
  as_tibble() %>% # convert to tibble
  set_names("word") %>% 
  mutate(word = str_replace_all(word, "\\(\\)", "")) %>% # remove special char
  mutate(word = str_replace_all(word, ",", "")) %>% 
  mutate(word = str_replace_all(word, "\\(", "")) %>% 
  mutate(word = str_replace_all(word, "\\)", "")) %>% 
  mutate(word = str_replace_all(word, "[.]", "")) %>% 
  mutate(word = tolower(word)) %>% 
  # remove a few words
  subset(word != "the" & word != "of" & word != "and" & word != "south" & word != "north" & word != "central") %>% 
  # add a few country names which have been split
  rbind("vietnam") %>% 
  rbind("ivory") %>% 
  rbind("ivoire") %>% 
  # and continents 
  rbind("Africa") %>% 
  rbind("Europe") %>% 
  rbind("Asia") %>% 
  mutate(type = "country") %>% # add type 
  mutate(word = str_to_lower(word))

# city names 
global_cities = read.csv("input_data/global_cities.csv", sep = ";") 
global_cities = global_cities[,2:3] %>% # keep only the english name and name in ASCII characters
  pivot_longer(cols = everything(),
               names_to = "type",
               values_to = "word") # put everything in one column
global_cities = global_cities[,2] %>% # only keep the words column
  mutate(word = str_split(word, pattern = " ")) %>% 
  unlist() %>% 
  as.data.frame() %>%   
  mutate(type = "location") # add type 
row.names(global_cities) = NULL # rename rows 
colnames(global_cities) = c("word", "type") # rename columns

# get list of 1st level administrative division in each country
global_admin = read.csv("input_data/global_admin.csv", sep = ",") %>% 
  mutate(name = str_split(name, pattern = " ")) %>% 
  unlist() %>% 
  as.data.frame() %>% 
  mutate(type = "location")
row.names(global_admin) = NULL # rename rows
colnames(global_admin) = c("word", "type") # rename columns

# now merge all the location names 
locations = global_cities %>% # cities
  rbind(global_admin) %>% # 1st level division
  mutate(word = str_to_lower(word)) %>% # to lower case
  unique() # keep unique names
```

### Tokenize data

#### 1. English requests

```{r def_lemmatisation}
lemmatisation_en = function(fid, cityname, rivername){
  print(paste0(fid, ": ", cityname))
  
  file = glue::glue("collected_data/english/scrap_tokens/tokens_{fid}.csv")
  
  if(!file.exists(file)){
    # read data
    my_df = read.csv(glue::glue("collected_data/english/scrap_translated/scrap_t_{fid}.csv", fileEncoding = "UTF-8")) %>% 
      mutate(text_en = case_when(
        # if the website is one of the following, then the text will be the snippet (more relevant)
        str_detect(domain, "books.google") ~ trans_snippet,
        str_detect(domain, "youtube") ~ trans_snippet,
        str_detect(domain, "alamy") ~ trans_snippet,
        str_detect(domain, "istockphoto") ~ trans_snippet,
        str_detect(domain, "gettyimages") ~ trans_snippet,
        str_detect(domain, "virtuoso") ~ trans_snippet,
        TRUE ~ text_en))
      
    # tokenize data
    df = my_df %>% 
      unnest_tokens(output = "word", input = "text_en", token = "words", drop = FALSE)  %>% 
      mutate(word = str_replace_all(word, "river's", "river")) %>% # replace a few words with the 's
      mutate(word = str_replace_all(word, "world's", "world")) %>% 
      mutate(word = str_replace_all(word, "oceangoing", "ocean-going")) %>% 
      mutate(word = str_replace_all(word, "seatrout", "sea-trout")) %>% 
      mutate(word = str_replace_all(word, "world's", "world")) %>% 
      mutate(word = str_replace_all(word, "can't", "can_t")) %>%  # replace ' by _ to match the lexicon 
      mutate(word = str_replace_all(word, "there's", "there_s")) %>% 
      mutate(word = str_replace_all(word, "doesn't", "doesn_t")) %>% 
      mutate(word = str_replace_all(word, "aren't", "aren_t")) %>% 
      mutate(word = str_replace_all(word, "didn't", "didn_t")) %>%
      mutate(word = str_replace_all(word, "don't", "don_t")) %>% 
      mutate(word = str_replace_all(word, "wasn't", "wasn_t")) %>% 
      mutate(word = str_replace_all(word, "they're", "they_re")) %>% 
      left_join(lexicon_en, by = "word") # join with english lexicon to get the lemma
      print("data tokenized") 
      # define words to remove, which are specific to this city
      # i.e. the city name and the river name
      fid_names = data.frame(word = c(cityname, rivername)) %>% 
        mutate(word = str_split(word, pattern = "OR ")) %>% 
        unlist() %>% 
        as.data.frame() 
      row.names(fid_names) = NULL
      colnames(fid_names) = "word" 
      
      fid_names = fid_names %>% 
        mutate(word = str_split(word, pattern = " ")) %>% 
        unlist() %>% 
        as.data.frame()
      row.names(fid_names) = NULL
      colnames(fid_names) = "word" 
      fid_names = fid_names %>% 
        mutate(word = str_to_lower(word)) %>% 
        subset(word != "")
      
      # now get all spellings of the city and river names 
      # example for N'Djaména
      fid_names = fid_names %>% 
        mutate(ascii = tolower(stringi::stri_trans_general(word, "latin-ascii"))) %>% # n'djamena
        mutate(punct = tolower(gsub('[[:punct:]]+', '', word))) %>% # ndjaména
        mutate(punct_ascii = tolower(gsub('[[:punct:]]+', '', stringi::stri_trans_general(word, "latin-ascii")))) %>% # ndjamena
        mutate(punct2 = tolower(gsub('[[:punct:] ]+', ' ', word))) %>% # n djaména
        mutate(punct2_ascii = tolower(gsub('[[:punct:] ]+', ' ', stringi::stri_trans_general(word, "latin-ascii")))) # n djamena 
      rows = fid_names %>% 
        # pivot the columns to rows
        pivot_longer(cols = everything(),
                     names_to = "type",
                     values_to = "word")
      # delete the first columns which doesn't interest us
        fid_names = rows[,2, drop = FALSE] %>% 
          unique()

      # clean data
      df_clean = df %>% 
        anti_join(stop_words, by = "word") %>% # remove stop words
        anti_join(fid_names, by = "word") %>% # remove city and river names
        anti_join(dates, by = "word") %>% # remove days and months
        anti_join(country_names, by = "word") %>% # remove country names
        filter(word != "river" & word != "character") %>% # remove "river" and "character" which is due to a translation error
        mutate(word = gsub("[0-9]", "", word)) %>% # remove numbers
        mutate(word = gsub("\\b[a-zA-Z]\\b", "", word)) %>% # remove solitary letters
        left_join(locations, by = "word", keep = FALSE) %>% # join with location names 
        # define which type to keep 
        mutate(type = case_when(
          !is.na(lemma) ~ type.x,
          is.na(lemma) & !is.na(type.y) ~ type.y, 
          is.na(lemma) & is.na(type.y) ~ NA
        )) %>% 
        # remove words which have no lemma and are not a location
        # we can assume that they are names of persons
        subset(!is.na(type)) %>% 
        # remove empty lines
        subset(word != "") %>% 
        # define which word to keep (lemma or word)
        mutate(token = case_when(
          !is.na(lemma) ~ lemma,
          is.na(lemma) ~ word)) 
      print("data cleaned")
      
      # now combine all the tokens to obtain a "tokenized" text
      tokenized_text = ddply(df_clean, "position", summarize, tokenized_text = paste(token, collapse = " "))
      # combine all the tokens without the location type
      df_without_loc = subset(df_clean, df_clean$type != "location")
tokenized_text_loc = ddply(df_without_loc, "position", summarize, tokenized_noloc = paste(token, collapse = " "))
      # join the resulting "tokenized texts" with the original dataframe
      my_df = my_df %>% 
        left_join(tokenized_text, by = "position") %>% 
        left_join(tokenized_text_loc, by = "position")

      write.csv(my_df, glue::glue("collected_data/english/scrap_tokens/tokens_{fid}.csv"), row.names = FALSE,  fileEncoding = "UTF-8")
   }
}
```

```{r run_get_serp_data_en, warning = FALSE, message = FALSE}
data_city_river %>% 
  mutate(data = purrr::pmap(list(fid = fid,
                                 cityname = city_en,
                                rivername = river_en
                               ),
                          lemmatisation_en))
```

#### 2. Requests in the local languages

```{r def_lemmatisation_hl}
lemmatisation_hl = function(fid, cityname, rivername, hl){
  
   file = glue::glue("collected_data/hl/tokens/tokens_{fid}_{hl}.csv")
    if(!file.exists(file)){
      
      # read data
      if(file.exists(glue::glue("collected_data/hl/clean_data/clean_{fid}_{hl}.csv"))){
      df = read.csv(glue::glue("collected_data/hl/clean_data/clean_{fid}_{hl}.csv")) 
      
      # tokenize data
      df = df %>% 
        unnest_tokens(output = "word", input = "trans_snippet", token = "words", drop = FALSE)  %>% 
        mutate(word = str_replace_all(word, "river's", "river")) %>% # replace a few words with the 's
        mutate(word = str_replace_all(word, "world's", "world")) %>% 
        mutate(word = str_replace_all(word, "oceangoing", "ocean-going")) %>% 
        mutate(word = str_replace_all(word, "seatrout", "sea-trout")) %>% 
        mutate(word = str_replace_all(word, "world's", "world")) %>% 
        mutate(word = str_replace_all(word, "can't", "can_t")) %>%  # replace ' by _ to match the lexicon 
        mutate(word = str_replace_all(word, "there's", "there_s")) %>% 
        mutate(word = str_replace_all(word, "doesn't", "doesn_t")) %>% 
        mutate(word = str_replace_all(word, "aren't", "aren_t")) %>% 
        mutate(word = str_replace_all(word, "didn't", "didn_t")) %>%
        mutate(word = str_replace_all(word, "don't", "don_t")) %>% 
        mutate(word = str_replace_all(word, "wasn't", "wasn_t")) %>% 
        mutate(word = str_replace_all(word, "they're", "they_re")) %>% 
        left_join(lexicon_en, by = "word") # join with english lexicon to get the lemma
          
      # define words to remove, which are specific to this city
      # i.e. the city name and the river name
      fid_names = data.frame(word = c(cityname, rivername)) %>% 
        mutate(word = str_split(word, pattern = "OR ")) %>% 
        unlist() %>% 
        as.data.frame() 
      row.names(fid_names) = NULL
      colnames(fid_names) = "word" 
      
      fid_names = fid_names %>% 
        mutate(word = str_split(word, pattern = " ")) %>% 
        unlist() %>% 
        as.data.frame()
      row.names(fid_names) = NULL
      colnames(fid_names) = "word" 
      fid_names = fid_names %>% 
        mutate(word = str_to_lower(word)) %>% 
        subset(word != "")
      
      # now get all spellings of the city and river names 
      # example for N'Djaména
      fid_names = fid_names %>% 
        mutate(ascii = tolower(stringi::stri_trans_general(word, "latin-ascii"))) %>% # n'djamena
        mutate(punct = tolower(gsub('[[:punct:]]+', '', word))) %>% # ndjaména
        mutate(punct_ascii = tolower(gsub('[[:punct:]]+', '', stringi::stri_trans_general(word, "latin-ascii")))) %>% # ndjamena
        mutate(punct2 = tolower(gsub('[[:punct:] ]+', ' ', word))) %>% # n djaména
        mutate(punct2_ascii = tolower(gsub('[[:punct:] ]+', ' ', stringi::stri_trans_general(word, "latin-ascii")))) # n djamena 
      fid_names = fid_names %>% 
        # pivot the columns to rows
        pivot_longer(cols = everything(),
                     names_to = "type",
                     values_to = "word")
      # delete the first columns which doesn't interest us
        fid_names = fid_names[,2] %>% 
          unique() %>% 
          rbind("river")

      # clean data
      df_clean = df %>% 
        anti_join(stop_words, by = "word") %>% # remove stop words
        anti_join(fid_names, by = "word") %>% # remove city and river names
        anti_join(dates, by = "word") %>% # remove days and months
        anti_join(country_names, by = "word") %>% # remove country names
        mutate(word = gsub("[0-9]", "", word)) %>% # remove numbers
        mutate(word = gsub("\\b[a-zA-Z]\\b", "", word)) %>% # remove solitary letters
        left_join(locations, by = "word", keep = FALSE) %>% # join with location names 
        # define which type to keep 
        mutate(type = case_when(
          !is.na(lemma) ~ type.x,
          is.na(lemma) & !is.na(type.y) ~ type.y, 
          is.na(lemma) & is.na(type.y) ~ NA
        )) %>% 
        # remove words which have no lemma and are not a location
        # we can assume that they are names of persons
        subset(!is.na(type)) %>% 
        # remove empty lines
        subset(word != "") %>% 
        # define which word to keep (lemma or word)
        mutate(token = case_when(
          !is.na(lemma) ~ lemma,
          is.na(lemma) ~ word)) 
      
      write.csv(df_clean, glue::glue("collected_data/hl/tokens/tokens_{fid}_{hl}.csv"), row.names = FALSE)
      print(paste0("Lemmatization done. FID = ", fid))
      }
      else {print(paste0("No data, FID = ", fid))}
   }
}
```

```{r run_functions, warning = FALSE}

for(i in 1:6) {
  n_hl = 7 + i 
  data_subset = filter(data_city_river, ((data_city_river[, n_hl] != "no data") & ((data_city_river[, n_hl] != "en") | data_city_river[, 7] != "us")))
  data = select(.data = data_subset, c(1:7, n_hl, 14, 21, 28, 34, 35))
  colnames(data) = c("fid", "latitude", "longitude", "urban_aggl", "country_en", "country_fr", "gl", "hl", "cityname", "rivername", "multiple_rivers", "tld", "google_domain")

data %>%
  mutate(data = purrr::pmap(list(fid = fid,
                                 cityname = cityname,
                                 rivername = rivername,
                                 hl = hl),
                            lemmatisation_hl))
}
```
