---
title: "Clustering results from the webscraping"
format: html
editor: visual
author: "Liolia Bajemon"
---

### Description

The aim of this document is to do a clusterisation on a corpus.

This method uses the [rainette package](https://juba.github.io/rainette/), which is based on a variant of the Reinert textual clustering method. This classification method is used for short text segments, which applies to our case, the snippets being approximately thirty words long.

### Set-up

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(magrittr)
library(tidyr)
library(plyr)
library(purrr)
library(stringr)
library(mixr)
library(tokenizers)
library(lexicon)
library(dplyr)
library(quanteda)
library(rainette)
library(rlist)
library(tmaptools)
library(ggplot2)
library(shiny)
library(FactoMineR)
library(quanteda.textplots)
library(RColorBrewer)
set.seed(1204) # set seed to get reproductible results
```

```{r def_palette}
# def palette
mypalette = get_brewer_pal("Set3", 9) 
mypalette[2] = "#e6ab02" # replace pale yellow 
mypalette[9] = "#bc80bd" # replace gray
big_palette = c(mypalette, mypalette)
```

### Clustering

```{r all_data}
df = read.csv("data_text.csv") %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% # rename column
  subset(!is.na(tokenized)) 

# read corpus
corpus = quanteda::corpus(df, text_field = "tokenized", docid_field = "id")
# split the corpus in smaller segments
corpus = split_segments(corpus, segment_size = 40)
tokens = tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, split_hyphens = TRUE) # tokenization and removal of diverse items

# calculate the document-feature matrix (DFM)
dfmatrix = dfm(tokens, remove_padding = TRUE) # remove NA or empty values
dfmatrix = dfm_trim(dfmatrix, min_docfreq = 10, min_termfreq = 200)

# we suggest to try multiple number of clusters and minimum split members 
# and choose what adapts best to your data
dhc = rainette(dfmatrix, k = 14, min_split_members = 500)
# OR
dhc = readRDS("dhc_rainette.rds")

plot = rainette_plot(
  dhc, 
  dfmatrix, 
  14,
  n_terms = 20,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Hydrographie,\ninondation", "Système\nd'alerte", "Mesures de\nqualité", "Pollution,\ntraitement des\neaux usées", "Aménagement,\ngestion,\nenvironnement", "Industrie", "Outlier\nInternet", "Infrastructures,\ntransports",  "Culture,\narchitecture", "Tourisme,\nnavigation", "Santé",  "Histoire,\nreligion", "Gouvernement,\narmée,\nconflits", "Education"),
  colors = big_palette
)
```

### 4. Plot specificities

```{r set_up_spec}
library(ggplot2)
library(ggalt)
library(proj4)
library(ggrepel)
library(rnaturalearth)
library(sf)
library(ggspatial)
```

We will use the resulting clusters and calculate their specificites, i.e. in which cities a given cluster has a high frequency.

```{r}
# using the mixr package, calculate the specificity for each city and each cluster:
spec_city = tidy_specificities(docvar, fid, cl) %>% 
  # values above 2 are significant (=< 1% probability)
  filter(spec >= 2) 
```

Now let's define a function which will plot the results for each cluster.

```{r def_plot_clusters}
plot_cluster = function(i){
  # get original dataset with map coordinates
  df = data_city_river %>% 
    # join with results
    left_join(spec_city, by = "fid") %>% 
    # only keep the ith cluster
    filter(cl == paste0("cl_",i)) %>% 
    mutate(longitude = str_replace_all(longitude, ",", ".")) %>% 
    mutate(latitude = str_replace_all(latitude, ",", ".")) %>% 
    mutate(latitude = as.numeric(latitude)) %>% 
    mutate(longitude = as.numeric(longitude))

  write.csv(df, paste0("analysis/clusters/spec_clusters/cluster_", i, ".csv"), row.names = FALSE)
  
  df = st_as_sf(df, coords = c("longitude", "latitude"), crs = 4326)
  df_coords = cbind(df, st_coordinates(df))

  world_map %>% 
    ggplot() +
    # world map 
    geom_sf(fill = "#f0f0f1", color = "white", size = 0.2) +
    # data points
    geom_sf(data = df, color = big_palette[i], size = 2) +
    geom_sf(data = df, color = "black", size = 0.5) +
    # river labels
    geom_text_repel(data = df, 
                  aes(label = riviere, geometry = geometry),
                  stat = "sf_coordinates", 
                  size = 3, 
                  nudge_x = 0.5, 
                  nudge_y = 0.5,
                  arrow = arrow(length = unit(0.2, "cm"), 
                                type = "closed"), 
                  max.overlaps = 25) +
    # north arrow
    annotation_north_arrow(location = "tl",
                           width = unit(0.8, "cm"),
                           height = unit(0.8, "cm"),
                           style = north_arrow_orienteering) +
    # equal earth projection
    coord_sf(crs = "+proj=eqearth") +
    # theme
    theme_void() +
    theme(legend.position = "bottom") +
    labs(title = paste0("Rivières pour lesquelles le sujet « ", labels[i], " » est abordé."),
         subtitle = "Score de spécificité ≥ 2")
  
ggsave(paste0("analysis/clusters/spec_clusters/carte_clusters_",i,".png"), width = 380, height = 200, units = "mm", limitsize = FALSE)
}
```

Run the function:

```{r}
# define world map
world_map = ne_countries(scale = "medium", returnclass = "sf")
# define labels
labels = c("Hydrographie,\ninondation", "Système\nd'alerte", "Mesures de\nqualité", "Pollution,\ntraitement des\neaux usées", "Aménagement,\ngestion,\nenvironnement", "Industrie", "Outlier\nInternet", "Infrastructures,\ntransports",  "Culture,\narchitecture", "Tourisme,\nnavigation", "Santé",  "Histoire,\nreligion", "Gouvernement,\narmée,\nconflits", "Education")

for(i in 1:14){ # 14 corresponds to the number of clusters
  plot_cluster(i)
}
```

```{r}
#test
df = read.csv("collected_data/english/scrap_tokens/tokens_832_a.csv") %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% # rename column
  subset(!is.na(tokenized)) 

# lecture du corpus
corpus = read_corpus(df, text_field = "tokenized", docid_field = "position")
# découpage du corpus
corpus_split = split_segments(corpus, segment_size = 40)
# transformation des segments en dataframe
corpus_segmented_df <- convert(corpus_split, to = "data.frame")
docvar = docvars(corpus_split) 

tokens = read_tokens(corpus)

dfmatrix = calc_dtm(tokens)

dhc = rainette(dfmatrix, k = 14, min_split_members = 500)

corpus$cluster = cutree(dhc, 14)

docvar = docvars(corpus) #%>% 
  mutate(cl = paste0("cl_", cluster)) 

```
