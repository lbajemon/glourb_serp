---
nt=---
title: "Clustering results from the webscraping"
format: html
editor: visual
author: "Liolia Bajemon"
---

### Description

The aim of this document is to do a clusterisation on a corpus.

This method uses the [rainette package](https://juba.github.io/rainette/), which is based on a variant of the Reinert textual clustering method. This classification method is used for short text segments, which applies to our case, the snippets being approximately thirty words long.

### Set-up

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(magrittr)
library(tidyr)
library(plyr)
library(purrr)
library(stringr)
library(mixr)
library(tokenizers)
library(lexicon)
library(dplyr)
library(quanteda)
library(rainette)
library(rlist)
library(tmaptools)
library(ggplot2)
library(shiny)
library(FactoMineR)
library(quanteda.textplots)
library(RColorBrewer)
library(sf)
library(rnaturalearth)
set.seed(1204) # set seed to get reproductible results

data_city_river = read.csv("input_data/data_city_river.csv", sep = ",") 
```

### 1. Read corpus

First, let's import our dataset and convert it to a corpus using the quanteda package:

```{r def_read_corpus}
read_corpus = function(df, text_field, docid_field){
  my_corpus = quanteda::corpus(df, docid_field, text_field)
  }
```

```{r def_tokens}
read_tokens = function(corpus){
  tok_serp = tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, split_hyphens = TRUE) # tokenization and removal of diverse items
  }
```

The tokenized snippets are already quite short, so we can skip the step of splitting the text in segments (using `split_segments`) and move on to the calculation of the document-feature matrix (dfm). The dfm is a mathematical matrix describing for each term, its frequency in each document. In rows are the documents (i.e. one row per snippet) and in columns are the terms.

### 2. Define function for clustering

```{r def_calc_dtm}
calc_dtm = function(tokens){
  # calculate the DFM
  dfmatrix = dfm(tokens, remove_padding = TRUE) # remove NA or empty values
  # remove feature which appear in a given number of documents 
  # and at least a given number of times 
  dfmatrix = dfm_trim(dfmatrix, min_docfreq = 10, min_termfreq = 200)
}
```

### 3. Run functions

```{r all_data}
df = read.csv("collected_data/corpus_distinct_2025.csv") %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% # rename column
  subset(!is.na(tokenized)) 

# read corpus
corpus = read_corpus(df, text_field = "tokenized", docid_field = "id")
# split the corpus in smaller segments
corpus = split_segments(corpus, segment_size = 40)
# saveRDS(corpus, "analysis/clusters/split_corpus_all.rds")
# corpus = readRDS("analysis/clusters/split_corpus_all.rds")
tokens = read_tokens(corpus)

# calculate the document-feature matrix (DFM)
dfmatrix = calc_dtm(tokens)

# we suggest to try multiple number of clusters and minimum split members 
# and choose what adapts best to your data
dhc = rainette(dfmatrix, k = 14, min_split_members = 500)
rainette_explor(dhc, dfmatrix, corpus)
saveRDS(dhc, "analysis/clusters/dhc_all.rds")
# dhc = readRDS("analysis/clusters/dhc_all.rds")

plot_en = rainette_plot(
  dhc, 
  dfmatrix, 
  14,
  n_terms = 20,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 14,
  cluster_label = c("Health", "Politics,\nconflicts", "Culture,\nknowledge", "Daily life,\nnews items", "Tourism,\ntravel",  "Heritage,\nattractions", "Urban\n infrastructure\nand geography", "Online images,\nweb content", "Planning,\nmanagement,\nenvironment", "Pollution,\nsewerage", "Physico-chemical\n parameters", "Alert system", "Floods", "Hydrography"),
  colors = c("#eac1f7","#fdb462", "#9e0142", "#f781bf", "#bc80bd" , "#bf812d" , "#f7bfbe", "#adadad","#b3de69", "#ffd92f","#8dd3c7", "#fb8072", "#bebada", "#80b1d3")
)
ggsave("analysis/clusters/clusters_all_14_en.png", plot_en, width = 500, height = 250, units = "mm", limitsize = FALSE)

plot_en = rainette_plot(
  dhc, 
  dfmatrix, 
  14,
  n_terms = 20,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 14,
  cluster_label = c("Health", "Politics,\nconflicts", "Culture,\nknowledge", "Daily life,\nnews items", "Tourism,\ntravel",  "Heritage,\nattractions", "Urban\n infrastructure\nand geography", "Online images,\nweb content", "Planning,\nmanagement,\nenvironment", "Pollution,\nsewerage", "Physico-\nchemical\n parameters", "Alert system", "Floods", "Hydrography"),
  colors = c("black","black", "black","black", "black","black", "black","black","black","black","black","black","black","black")
)
ggsave("analysis/clusters/clusters_article.svg", plot_en, width = 500, height = 250, units = "mm", limitsize = FALSE)

plot_fr = rainette_plot(
  dhc, 
  dfmatrix, 
  14,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Santé", "Gouvernement,\n conflits", "Culture,\nconnaissances", "Vie quotidienne,\nfaits divers", "Tourisme,\nvoyage", "Patrimoine,\nattractions", "Infrastructures\net géographie\nurbaines", "Banques d'images,\ncontenu web", "Planification,\ngouvernance,\nenvironnement", "Pollution,\ntraitement des\neaux usées","Paramètres\nphysico-\nchimiques", "Système\nd'alerte", "Inondations,\nquantité", "Hydrographie")
,
  colors = c("#eac1f7","#fdb462", "#9e0142", "#f781bf", "#bc80bd" , "#bf812d" , "#f7bfbe", "#adadad","#b3de69", "#ffd92f","#8dd3c7", "#fb8072", "#bebada", "#80b1d3")
)
ggsave("analysis/clusters/clusters_all_14_fr.svg", plot_fr, width = 500, height = 250, units = "mm", limitsize = FALSE)
ggsave("analysis/clusters/clusters_all_14_fr.png", plot_fr, width = 500, height = 250, units = "mm", limitsize = FALSE)


### 4 topics
plot_en = rainette_plot(
  dhc, 
  dfmatrix, 
  4,
  n_terms = 20,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 15,
  cluster_label = c("Politics", "Leisure", "Water quality", "Water quantity"),
  colors = c("#fdb462", "#f781bf", "#b3de69", "#8dd3c7")
)
ggsave("analysis/clusters/clusters_4_article.svg", plot_en, width = 500, height = 200, units = "mm", limitsize = FALSE)
ggsave("analysis/clusters/clusters_4.png", plot_en, width = 500, height = 250, units = "mm", limitsize = FALSE)
```

#### A topic per document

```{r}
# write topics according to classification 
corpus$topic = cutree(dhc, 14)
# and turn it into a df, keep only two variables for the specificity calculation
corpus_df = convert(corpus, to = "data.frame") %>% 
  select(c("segment_source", "topic")) %>% 
  mutate(top = paste0("_", topic))

# now calculate get the most specific cluster for each document
spec_doc = tidy_specificities(corpus_df, segment_source, cl) %>% 
# keep only the biggest value for each document
  group_by(segment_source) %>% 
  filter(spec == max(spec)) %>% 
  ungroup() 
# if the specificity is < 2, then it is not significant. Then we will keep the cluster which is the largest.
# if the document contains only one segment, then its cluster doesn't require any calculation. 
max_doc = corpus_df %>% 
  group_by(cl, segment_source) %>% 
  dplyr::summarise(count = n(), .groups = "drop") %>% 
  group_by(segment_source) %>% 
  filter(count == max(count))
max_doc = dplyr::rename(max_doc, cl_max = cl)
max_doc = dplyr::rename(max_doc, count_max = count)

spec = spec_doc %>% 
  left_join(max_doc, by = "segment_source") %>% 
  mutate(cluster = case_when(n == 1 ~ cl_max,
                        spec < 2 ~ cl_max,
                        TRUE ~ cl)) 
# Note: a document can have two clusters. This happens when there is an equality (for example if two segments fall in cluster 1 and two segments fall in cluster 2, the document will be cluster 1 AND 2)
```

### 4. Plot specificities

```{r set_up_spec}
library(ggplot2)
library(ggalt)
library(proj4)
library(ggrepel)
library(rnaturalearth)
library(sf)
library(ggspatial)
library(forcats)
```

We will use the resulting clusters and calculate their specificites, i.e. in which cities a given cluster has a high frequency.

```{r spec}
# write clusters on the corpus
corpus$topic = cutree(dhc, 14)
# and turn it into a df
corpus_segments_df = convert(corpus, to = "data.frame")
# simplify the dataframe
corpus_segments_df = corpus_segments_df %>% 
  select(c("text", "fid", "link", "segment_source", "text_en", "topic", "hl", "query")) %>% 
  mutate(cl = paste0("topic_", topic))

# the file is too big so we will divide it by city
docvar_city = function(fid_city){
  if(!file.exists(glue::glue("analysis/clusters/topic_city/topics_{fid_city}.csv"))){
  df_city = corpus_segments_df %>% 
    filter(fid == fid_city) 
  print(fid_city)
  write.csv(df_city, glue::glue("analysis/clusters/topic_city/topics_{fid_city}.csv"), row.names = FALSE)
  }
}
# rename column
data_city_river = data_city_river %>% 
  rename("fid_city" = "fid")
# run the function 
data_city_river %>% 
  mutate(data = purrr::map(fid_city, docvar_city))

# we also divide the corpus by cluster
docvar_cl = function(i){
  if(!file.exists(glue::glue("analysis/clusters/df_by_cluster/df_cluster_{i}.csv"))){
    df = corpus_segments_df %>% 
      filter(cluster == i) 
    write.csv(df, glue::glue("analysis/clusters/df_by_cluster/df_cluster_{i}.csv"), row.names = FALSE)
    }
  }

  for(i in 1:14){
    docvar_cl(i)
    print(i)
  }
```

```{r}
# using the mixr package, calculate the specificity for each city and each cluster:
spec_city = tidy_specificities(corpus_segments_df, fid, cl) %>% 
  # values above 2 are significant (=< 1% probability)
  filter(spec >= 2) 
```

Now let's define a function which will plot the results for each cluster.

```{r def_plot_clusters}
plot_cluster = function(i){
  # get original dataset with map coordinates
  df = data_city_river %>% 
    # join with results
    left_join(spec_city, by = "fid") %>% 
    # only keep the ith cluster
    filter(cl == paste0("cl_",i)) %>% 
    mutate(longitude = str_replace_all(longitude, ",", ".")) %>% 
    mutate(latitude = str_replace_all(latitude, ",", ".")) %>% 
    mutate(latitude = as.numeric(latitude)) %>% 
    mutate(longitude = as.numeric(longitude))

  write.csv(df, paste0("analysis/clusters/spec_clusters/cluster_", i, ".csv"), row.names = FALSE)
  
  df = st_as_sf(df, coords = c("longitude", "latitude"), crs = 4326)
  df_coords = cbind(df, st_coordinates(df))

  world_map %>% 
    ggplot() +
    # world map 
    geom_sf(fill = "#f0f0f1", color = "white", size = 0.2) +
    # data points
    geom_sf(data = df, color = big_palette[i], size = 2) +
    geom_sf(data = df, color = "black", size = 0.5) +
    # river labels
    geom_text_repel(data = df, 
                  aes(label = riviere, geometry = geometry),
                  stat = "sf_coordinates", 
                  size = 3, 
                  nudge_x = 0.5, 
                  nudge_y = 0.5,
                  arrow = arrow(length = unit(0.2, "cm"), 
                                type = "closed"), 
                  max.overlaps = 25) +
    # north arrow
    annotation_north_arrow(location = "tl",
                           width = unit(0.8, "cm"),
                           height = unit(0.8, "cm"),
                           style = north_arrow_orienteering) +
    # equal earth projection
    coord_sf(crs = "+proj=eqearth") +
    # theme
    theme_void() +
    theme(legend.position = "bottom") +
    labs(title = paste0("Rivières pour lesquelles le sujet « ", labels[i], " » est abordé."),
         subtitle = "Score de spécificité ≥ 2")
  
ggsave(paste0("analysis/clusters/spec_clusters/carte_clusters_",i,".png"), width = 380, height = 200, units = "mm", limitsize = FALSE)
}
```

Run the function:

```{r}
# define world map
world_map = ne_countries(scale = "medium", returnclass = "sf")
# define labels
labels = c("Hydrographie,\ninondation", "Système\nd'alerte", "Mesures de\nqualité", "Pollution,\ntraitement des\neaux usées", "Aménagement,\ngestion,\nenvironnement", "Industrie", "Outlier\nInternet", "Infrastructures,\ntransports",  "Culture,\narchitecture", "Tourisme,\nnavigation", "Santé",  "Histoire,\nreligion", "Gouvernement,\narmée,\nconflits", "Education")

for(i in 1:14){ # 14 corresponds to the number of clusters
  plot_cluster(i)
}
```

#### Map specificity

##### 14 clusters

```{r}
world_map = ne_countries(scale = "medium", returnclass = "sf")
topic_names = read.csv("input_data/topic_names.csv", sep = ",") %>% 
  select(c("topic", "name", "couleur"))
coords = data_city_river %>% 
  select(c("fid", "latitude", "longitude")) %>% 
  mutate(longitude = as.numeric(str_replace_all(longitude, ",", "."))) %>% 
  mutate(latitude = as.numeric(str_replace_all(latitude, ",", "."))) 

# write clusters on the corpus
corpus$topic = cutree(dhc, 14)
# and turn it into a df
corpus_segments_df = convert(corpus, to = "data.frame")
# simplify the dataframe
corpus_segments_df = corpus_segments_df %>% 
  select(c("text", "fid", "link", "segment_source", "text_en", "topic", "hl", "query")) %>% 
  mutate(topic = paste0("topic_", topic))

spec_city = tidy_specificities(corpus_segments_df, fid, topic) %>% 
  # values above 2 are significant (=< 1% probability)
  filter(spec >= 2) %>% 
  group_by(fid) %>% 
  filter(spec == max(spec)) %>% 
  filter(n == max(n)) %>% 
  ungroup() %>% 
  left_join(coords, by = "fid") %>% 
  left_join(topic_names, by = "topic")

# map
 df = st_as_sf(spec_city, coords = c("longitude", "latitude"), crs = 4326)
 df_coords = cbind(df, st_coordinates(df))

 col_vec = setNames(df$couleur, df$name)

  world_map %>% 
    ggplot() +
    # world map 
    geom_sf(fill = "#f0f0f1", color = "white", size = 0.2) +
    # data points
    geom_sf(data = df, aes(color = name), size = 2) +
    scale_color_manual(values = col_vec) +
    # equal earth projection
    coord_sf(crs = "+proj=eqearth") +
    # theme
    theme_void() +
    theme(legend.position = "bottom")
  
  ggsave(paste0("analysis/clusters/specificite_14.svg"), width = 380, height = 200, units = "mm", limitsize = FALSE)
```

##### 4 clusters

```{r}
fid_fusion = read.csv("input_data/data_city_river_fid_fusion.csv")
world_map = ne_countries(scale = "medium", returnclass = "sf")
topic_names = read.csv("input_data/topic_names4.csv") %>% 
  select(c("topic", "nom", "col"))
coords = read.csv("input_data/data_city_river_303.csv") %>% 
  select(c("fid", "latitude", "longitude")) %>% 
  mutate(longitude = as.numeric(str_replace_all(longitude, ",", "."))) %>% 
  mutate(latitude = as.numeric(str_replace_all(latitude, ",", "."))) %>% 
  mutate(fid = as.character(fid))

# write clusters on the corpus
corpus$topic = cutree(dhc, 4)
# and turn it into a df
corpus_segments_df = convert(corpus, to = "data.frame")
# simplify the dataframe
corpus_segments_df = corpus_segments_df %>% 
  left_join(fid_fusion, by = "fid") %>% 
  select(c("text", "fid_fusion", "link", "segment_source", "text_en", "topic", "hl", "query")) %>% 
  mutate(topic = paste0("topic_", topic)) %>% 
  rename("fid" = "fid_fusion") %>% 
  mutate(fid = as.character(fid))

spec_city = tidy_specificities(corpus_segments_df, fid, topic) %>% 
  # values above 2 are significant (=< 1% probability)
  group_by(fid) %>% 
  filter(spec == max(spec)) %>% 
  filter(n == max(n)) %>% 
  ungroup() %>% 
  left_join(coords, by = "fid") %>% 
  left_join(topic_names, by = "topic") %>% 
  mutate(nom = ifelse(spec<2|is.na(nom), "Non significatif", nom),
         col = ifelse(nom=="Non significatif", "grey", col))

# map
 df = st_as_sf(spec_city, coords = c("longitude", "latitude"), crs = 4326)
 df_coords = cbind(df, st_coordinates(df))

 col_vec = setNames(df$col, df$nom)
 
 world_map %>% 
    ggplot() +
    # world map 
    geom_sf(fill = "#f0f0f1", color = "white", size = 0.2) +
    # data points
    geom_sf(data = df, aes(color = nom,
                           size = spec), alpha = 0.7) +
   scale_color_manual(values = col_vec,
                      breaks = c("Société", "Loisirs", "Qualité de l’eau", "Quantité d’eau", "Non significatif")) +    
   # equal earth projection
    coord_sf(crs = "+proj=eqearth") +
    annotation_north_arrow(location = "tr",
                           width = unit(1, "cm"),
                           height = unit(1, "cm"),
                           style = north_arrow_nautical(
                             fill = c('white', "white"),
                             line_col = "#d9d9d9",
                             text_col = "#d9d9d9")) +
    # theme
    theme_void() +
    labs(color = "Monde lexical\nle plus spécifique",
         size = "Score de spécificité",
         caption = "Réalisation : L. Bajemon, août 2025\nFond de carte : R Natural Earth") +
    theme(legend.position = "bottom",
          plot.caption = element_text(color = "#d9d9d9",hjust = 0.5),
          legend.box = "vertical")
  
ggsave("analysis/clusters/specificite_4.svg", width = 4000, height = 2500, units = "px")
```

### Summary

```{r}
# write lexical worlds 
corpus$topic4 = cutree(dhc, 4)
corpus$topic14 = cutree(dhc, 14)

# get a summary for each city
summary_4 = convert(corpus, to = "data.frame") %>% 
  select(c("fid", "topic4")) %>% 
  group_by(fid, topic4) %>% 
  # count number of segments by topic
  dplyr::mutate(n_topic4 = n()) %>% 
  distinct() %>% 
  group_by(fid) %>% 
  dplyr::mutate(n_tot = sum(n_topic4)) %>% 
  # count percentage of topics
  mutate(n_topic4 = n_topic4/n_tot*100) %>% 
  mutate(topic_4 = paste0("topic4_", topic4)) %>% 
  # pivot df to keep one row per city 
  pivot_wider(names_from = topic_4,
              id_cols = fid,
              values_from = n_topic4) %>% 
  replace(is.na(.), 0)

summary_14 = convert(corpus, to = "data.frame") %>% 
  select(c("fid", "topic14")) %>% 
  group_by(fid, topic14) %>% 
  dplyr::mutate(n_topic14 = n()) %>% 
  distinct() %>%
  group_by(fid) %>% 
  dplyr::mutate(n_tot = sum(n_topic14)) %>% 
  # count percentage of topics
  mutate(n_topic14 = n_topic14/n_tot*100) %>% 
  mutate(topic_14 = paste0("topic14_", topic14)) %>% 
  pivot_wider(names_from = topic_14,
              id_cols = fid,
              values_from = n_topic14) %>% 
  replace(is.na(.), 0)

write.csv(summary_4, "analysis/clusters/summary_4_topics.csv", row.names = FALSE)
write.csv(summary_14, "analysis/clusters/summary_14_topics.csv", row.names = FALSE)
```
