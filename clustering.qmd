---
nt=---
title: "Clustering results from the webscraping"
format: html
editor: visual
author: "Liolia Bajemon"
---

### Description

The aim of this document is to do a clusterisation on a corpus.

This method uses the [rainette package](https://juba.github.io/rainette/), which is based on a variant of the Reinert textual clustering method. This classification method is used for short text segments, which applies to our case, the snippets being approximately thirty words long.

### Set-up

```{r load_packages, results = 'hide', warning = FALSE, message = FALSE}
library(magrittr)
library(tidyr)
library(plyr)
library(purrr)
library(stringr)
library(mixr)
library(tokenizers)
library(lexicon)
library(dplyr)
library(quanteda)
library(rainette)
library(rlist)
library(tmaptools)
library(ggplot2)
library(shiny)
library(FactoMineR)
library(quanteda.textplots)
library(RColorBrewer)
set.seed(1204) # set seed to get reproductible results

data_city_river = read.csv("input_data/data_city_river.csv", sep = ",") 
```

```{r def_palette}
# def palette
big_palette = c("#8dd3c7", "#fdb462", "#bebada", "#ffed6f","#b3de69", "#bf812d", "#d9d9d9",  "#fccde5", "#bc80bd", "#80b1d3","#ccebc5","#9e0142", "#fb8072", "#f781bf", "#969696")
```

### 1. Read corpus

First, let's import our dataset and convert it to a corpus using the quanteda package:

```{r def_read_corpus}
read_corpus = function(df, text_field, docid_field){
  my_corpus = quanteda::corpus(df, docid_field, text_field)
  }
```

```{r def_tokens}
read_tokens = function(corpus){
  tok_serp = tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, split_hyphens = TRUE) # tokenization and removal of diverse items
  }
```

The tokenized snippets are already quite short, so we can skip the step of splitting the text in segments (using `split_segments`) and move on to the calculation of the document-feature matrix (dfm). The dfm is a mathematical matrix describing for each term, its frequency in each document. In rows are the documents (i.e. one row per snippet) and in columns are the terms.

### 2. Define functions for clustering

#### a. Calculate the DFM

```{r def_calc_dtm}
calc_dtm = function(tokens){
  # calculate the DFM
  dfmatrix = dfm(tokens, remove_padding = TRUE) # remove NA or empty values
  # remove feature which appear in a given number of documents 
  # and at least a given number of times 
  dfmatrix = dfm_trim(dfmatrix, min_docfreq = 10, min_termfreq = 200)
}
```

### 3. Run functions

#### a. For results in English

First, let's read our data, which corresponds to all the "tokenized snippets" for the queries in English (see [here](https://github.com/lbajemon/glourb_serp/blob/main/lemmatisation.qmd)). There are two different tokenized snippets for each element: one including locations (e.g. "France, "Lyon", *etc.*) and one excluding them. Through trial and error, we find that in this case it is better to exclude them from the corpus. For the calculation of the document-feature matrix, we remove features which appear in less than 10 documents and which appear less than 200 times.

```{r read_data_en}

# read datasets

df = read.csv("collected_data/english/scrap_data_river_en.csv") %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% # rename column
  subset(!is.na(tokenized)) %>% # remove empty values
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() 

# read corpus
corpus = read_corpus(df, text_field = "tokenized", docid_field = "id")
# split the corpus in smaller segments
corpus = split_segments(corpus, segment_size = 40)
tokens = read_tokens(corpus)

# calculate the document-feature matrix (DFM)
dfmatrix = calc_dtm(tokens)

# we suggest to try multiple number of clusters and minimum split members 
# and choose what adapts best to your data
dhc = rainette(dfmatrix, k = 17, min_split_members = 500)
rainette_explor(dhc, dfmatrix, corpus)

# plot the results
plot = rainette_plot(
  dhc, 
  dfmatrix, 
  17,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Santé", "Culture,\nreligion", "Politique,\ngouvernement,\nconflits", "Education", "Système d'alerte", "Mesures de qualité", "Hydrologie,\ninondation", "Académique", "Aménagement,\ngestion de la\nressource,\nenvironnement", "Pollutions,\neaux usées", "Outlier\n(commerces)", "Moyens de\ntransport", "Loisirs\naquatiques et\nnautiques", "Tourisme,\nloisirs,\nespaces verts", "Hydrographie,\nlocalisation", "Météo", "Outlier\n(images)"), 
  colors = big_palette
)
ggsave("analysis/clusters/clusters_en_17.png", plot, width = 500, height = 250, units = "mm", limitsize = FALSE)

  # write class of the doc according to the clustering
  corpus$cluster = cutree(dhc, 17)
  # create a df indicating the cluster for each doc 
  docvar = docvars(corpus) %>% 
    mutate(id = paste0(fid, "_", position)) %>% 
    # seg_id: unique identifier of each segment 
    subset(select = -c(1:(ncol(docvars(corpus))-1)))
  # join with our original dataset
  df_clusters = df %>% 
    left_join(docvar, by = "id", keep = FALSE)
  
  write.csv(df_clusters, "analysis/clusters/clusters_en_17.csv", row.names = FALSE)
```

#### b. For results in the first local language

Like for the English results, there are two different tokenized snippets for each element: one including locations (e.g. "France, "Lyon", *etc.*) and one excluding them. We find that excluding locations works better, because it eliminates the outliers. For the calculation of the document-feature matrix, we remove features which appear in less than 10 documents and which appear less than 200 times.

```{r read_data_hl1}
df_noloc = read.csv("collected_data/hl/df_scrap_tokens_hl1.csv") %>%
  dplyr::rename(id = snippet_id) %>% # rename doc id 
  select(-"tokenized_text") %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% 
  subset(!is.na(tokenized)) %>%  # remove empty values
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() 
corpus_noloc = read_corpus(df_noloc, text_field = "tokenized", docid_field = "id")
corpus_noloc = split_segments(corpus_noloc, segment_size = 40)
tokens_noloc = read_tokens(corpus_noloc)

dfmatrix_noloc = calc_dtm(tokens_noloc)

# Like for the English results 
# try multiple numbers of clusters
dhc_noloc = rainette(dfmatrix_noloc, k = 20, min_split_members = 500)
rainette_explor(dhc_noloc, dfmatrix_noloc, corpus_noloc)
```

Like for the English results, we will apply two clusterisations: one with 17 clusters and one with 8 clusters.

```{r 17_clusters_hl1}
dhc = rainette(dfmatrix_noloc, k = 17, min_split_members = 500)
rainette_explor(dhc, dfmatrix_noloc, corpus_noloc)

# plot the results
plot = rainette_plot(
  dhc, 
  dfmatrix_noloc, 
  17,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Inondation", "Ouvrages\nhydrauliques", "Hydrographie", "Météo", "Tourisme,\nloisirs,\nespaces verts",  "Navigation,\nmoyens de\ntransports", "Biodiversité,\npêche", "Noyade,\nvictimes", "Culture,\nmusées", "Armée,\nguerre", "Système\nd'alerte", "Industrie", "Projet\nd'aménagement", "Pollution,\ntraitement des\neaux usées", "Education", "Gouvernement", "Archive,\ninternet"), 
  colors = big_palette
)
ggsave("analysis/clusters/clusters_hl1_17.png", plot, width = 500, height = 250, units = "mm", limitsize = FALSE)

  # write class of the doc according to the clustering
  corpus_noloc$cluster = cutree(dhc, 17)
  # create a df indicating the cluster for each doc 
  docvar = docvars(corpus_noloc) %>% 
    mutate(id = paste0(fid, "_", position)) %>% 
    # seg_id: unique identifier of each segment 
    subset(select = -c(1:(ncol(docvars(corpus_noloc))-1)))
```

```{r 8_clusters_hl1}
dhc = rainette(dfmatrix_noloc, k = 8, min_split_members = 500)
rainette_explor(dhc, dfmatrix_noloc, corpus_noloc)

# plot the results
plot = rainette_plot(
  dhc, 
  dfmatrix_noloc, 
  8,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Système\nd'alerte", "Hydrographie,\ninondation", "Armée,\nguerre", "Tourisme,\nloisirs,\nnavigation", "Droit", "Projet\nd'aménagement,\npollution", "Gouvernement,\ninstitutions", "Industrie"), 
  colors = big_palette
)
ggsave("analysis/clusters/clusters_hl1_8.png", plot, width = 500, height = 250, units = "mm", limitsize = FALSE)
```

#### c. For results in all local languages

Now we will apply a clustering to all the results found from the queries in all local languages (up to 6 per city).

```{r read_data_hl}
df_loc = read.csv("collected_data/hl/df_tokenized_all.csv") %>% 
  dplyr::rename(seg_id = snippet_id) %>% # rename doc id 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() %>% 
  subset(!is.na(tokenized)) # remove empty values
df_noloc = read.csv("collected_data/hl/df_tokenized_all.csv") %>%
  dplyr::rename(seg_id = snippet_id) %>% # rename doc id 
  group_by(fid) %>% 
  dplyr::mutate(n = n()) %>% # count number of snippets per city 
  ungroup() %>% 
  subset(!is.na(tokenized_noloc)) # remove empty values

corpus_loc = read_corpus(df_loc, text_field = "tokenized", docid_field = "seg_id")
tokens_loc = read_tokens(corpus_loc)
corpus_noloc = read_corpus(df_noloc, text_field = "tokenized_noloc", docid_field = "seg_id")
tokens_noloc = read_tokens(corpus_noloc)

dfmatrix_loc = calc_dtm(tokens_loc)
dfmatrix_noloc = calc_dtm(tokens_noloc)

# try multiple numbers of clusters and choose the best one
dhc_loc = rainette(dfmatrix_loc, k = 20, min_split_members = 500)
rainette_explor(dhc_loc, dfmatrix_loc, corpus_loc)
dhc_noloc = rainette(dfmatrix_noloc, k = 20, min_split_members = 500)
rainette_explor(dhc_noloc, dfmatrix_noloc, corpus_noloc)
```

#### d. For results in English and in all local languages (i.e. everything combined)

```{r all_data}
df = read.csv("collected_data/corpus_distinct.csv") %>% 
  dplyr::rename(tokenized = tokenized_noloc) %>% # rename column
  subset(!is.na(tokenized)) 

# read corpus
corpus = read_corpus(df, text_field = "tokenized", docid_field = "id")
# split the corpus in smaller segments
corpus = split_segments(corpus, segment_size = 40)
saveRDS(corpus, "analysis/clusters/split_corpus_all.rds")
# corpus = readRDS("analysis/clusters/split_corpus_all.rds")
tokens = read_tokens(corpus)

# calculate the document-feature matrix (DFM)
dfmatrix = calc_dtm(tokens)

# we suggest to try multiple number of clusters and minimum split members 
# and choose what adapts best to your data
dhc = rainette(dfmatrix, k = 14, min_split_members = 500)
rainette_explor(dhc, dfmatrix, corpus)
saveRDS(dhc, "analysis/clusters/dhc_all.rds")
# dhc = readRDS("analysis/clusters/dhc_all.rds")

plot_en = rainette_plot(
  dhc, 
  dfmatrix, 
  14,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Health", "Government\nand conflicts", "University,\nhistory,\nreligion", "Daily life,\nnews items", "Tourism,\ntravel",  "Heritage,\nattractions", "Urban\n infrastructure\nand geography", "Online images,\nweb content", "Planning,\nmanagement,\nenvironment", "Pollution,\nsewerage", "Chemical and\nphysical\nparameters", "Alert system", "Floods", "Hydrography"),
  colors = c("#eac1f7","#fdb462", "#9e0142", "#f781bf", "#bc80bd" , "#bf812d" , "#f7bfbe", "#adadad","#b3de69", "#ffd92f","#8dd3c7", "#fb8072", "#bebada", "#80b1d3")
)
ggsave("analysis/clusters/clusters_all_14_en.png", plot_en, width = 500, height = 250, units = "mm", limitsize = FALSE)

plot_fr = rainette_plot(
  dhc, 
  dfmatrix, 
  14,
  n_terms = 30,
  free_scales = TRUE,
  measure = "chi2",
  show_negative = FALSE,
  text_size = 12,
  cluster_label = c("Santé", "Gouvernement\net conflits", "Université,\nhistoire,\nreligion", "Vie quotidienne,\nfaits divers", "Tourisme,\nvoyage", "Patrimoine,\nattractions", "Infrastructures\net géographie\nurbaines", "Banques d'images,\ncontenu web", "Aménagement,\ngestion,\nenvironnement", "Pollution,\ntraitement des\neaux usées","Paramètres\nchimiques\net physiques", "Système\nd'alerte", "Inondations", "Hydrographie")
,
  colors = c("#eac1f7","#fdb462", "#9e0142", "#f781bf", "#bc80bd" , "#bf812d" , "#f7bfbe", "#adadad","#b3de69", "#ffd92f","#8dd3c7", "#fb8072", "#bebada", "#80b1d3")
)
ggsave("analysis/clusters/clusters_all_14_fr.png", plot_fr, width = 500, height = 250, units = "mm", limitsize = FALSE)
```

### 4. Plot specificities

```{r set_up_spec}
library(ggplot2)
library(ggalt)
library(proj4)
library(ggrepel)
library(rnaturalearth)
library(sf)
library(ggspatial)
library(forcats)
```

We will use the resulting clusters and calculate their specificites, i.e. in which cities a given cluster has a high frequency.

```{r spec}
# write clusters on the corpus
corpus$cluster = cutree(dhc, 14)
# and turn it into a df
corpus_segments_df = convert(corpus, to = "data.frame")
# simplify the dataframe
corpus_segments_df = corpus_segments_df %>% 
  select(c("text", "fid", "link", "segment_source", "text_en", "cluster")) %>% 
  mutate(cl = paste0("cl_", cluster))

# the file is too big so we will divide it by city
docvar_city = function(fid_city){
  if(!file.exists(glue::glue("analysis/clusters/clusters_city/clusters_{fid_city}.csv"))){
  df_city = corpus_segments_df %>% 
    filter(fid == fid_city) 
  print(fid_city)
  write.csv(df_city, glue::glue("analysis/clusters/clusters_city/clusters_{fid_city}.csv"), row.names = FALSE)
  }
}
# run the function
data_city_river = data_city_river %>% 
  rename("fid_city" = "fid")
data_city_river %>% 
  mutate(data = purrr::map(fid_city, docvar_city))

# we also divide the corpus by cluster
docvar_cl = function(i){
  if(!file.exists(glue::glue("analysis/clusters/df_by_cluster/df_cluster_{i}.csv"))){
    df = corpus_segments_df %>% 
      filter(cluster == i) 
    write.csv(df, glue::glue("analysis/clusters/df_by_cluster/df_cluster_{i}.csv"), row.names = FALSE)
    }
  }

  for(i in 1:14){
    docvar_cl(i)
    print(i)
  }
```

```{r}
 library(mixr)
    lexicon_en = get_lexicon("en")
    wp_segments = petit_corpus %>%
    tidytext::unnest_tokens(output="word",input="text",token="words") %>%
    left_join(lexicon_en,by=c("word")) #%>%
    mutate(keep=(type!="sw" & lemma!="flood")) %>% 
    group_by(flood,article) %>%
    mutate(lemma=case_when(is.na(lemma)|(!keep)~"",
                           TRUE~lemma)) %>% 
    mutate(keep=as.numeric(keep)) %>% 
    mutate(num_lemma=case_when(is.na(keep)~0,
                     TRUE~keep)) %>% 
    mutate(num_lemma=cumsum(num_lemma)) %>% 
    mutate(num_segment=ceiling(num_lemma/10+0.000001)) %>% 
    ungroup() #%>% 
    group_by(flood,article,num_segment) %>% 
    summarise(text_all=paste0(word, collapse=" "),
              text_sig=paste0(lemma,collapse=" "),
              .groups="drop") %>% 
    mutate(num_segment=paste0(article,"_",num_segment)) %>% 
    mutate(text_sig_dontkeep=text_sig)
```

```{r}
# using the mixr package, calculate the specificity for each city and each cluster:
spec_city = tidy_specificities(corpus_segments_df, fid, cl) %>% 
  # values above 2 are significant (=< 1% probability)
  filter(spec >= 2) 
```

Now let's define a function which will plot the results for each cluster.

```{r def_plot_clusters}
plot_cluster = function(i){
  # get original dataset with map coordinates
  df = data_city_river %>% 
    # join with results
    left_join(spec_city, by = "fid") %>% 
    # only keep the ith cluster
    filter(cl == paste0("cl_",i)) %>% 
    mutate(longitude = str_replace_all(longitude, ",", ".")) %>% 
    mutate(latitude = str_replace_all(latitude, ",", ".")) %>% 
    mutate(latitude = as.numeric(latitude)) %>% 
    mutate(longitude = as.numeric(longitude))

  write.csv(df, paste0("analysis/clusters/spec_clusters/cluster_", i, ".csv"), row.names = FALSE)
  
  df = st_as_sf(df, coords = c("longitude", "latitude"), crs = 4326)
  df_coords = cbind(df, st_coordinates(df))

  world_map %>% 
    ggplot() +
    # world map 
    geom_sf(fill = "#f0f0f1", color = "white", size = 0.2) +
    # data points
    geom_sf(data = df, color = big_palette[i], size = 2) +
    geom_sf(data = df, color = "black", size = 0.5) +
    # river labels
    geom_text_repel(data = df, 
                  aes(label = riviere, geometry = geometry),
                  stat = "sf_coordinates", 
                  size = 3, 
                  nudge_x = 0.5, 
                  nudge_y = 0.5,
                  arrow = arrow(length = unit(0.2, "cm"), 
                                type = "closed"), 
                  max.overlaps = 25) +
    # north arrow
    annotation_north_arrow(location = "tl",
                           width = unit(0.8, "cm"),
                           height = unit(0.8, "cm"),
                           style = north_arrow_orienteering) +
    # equal earth projection
    coord_sf(crs = "+proj=eqearth") +
    # theme
    theme_void() +
    theme(legend.position = "bottom") +
    labs(title = paste0("Rivières pour lesquelles le sujet « ", labels[i], " » est abordé."),
         subtitle = "Score de spécificité ≥ 2")
  
ggsave(paste0("analysis/clusters/spec_clusters/carte_clusters_",i,".png"), width = 380, height = 200, units = "mm", limitsize = FALSE)
}
```

Run the function:

```{r}
# define world map
world_map = ne_countries(scale = "medium", returnclass = "sf")
# define labels
labels = c("Hydrographie,\ninondation", "Système\nd'alerte", "Mesures de\nqualité", "Pollution,\ntraitement des\neaux usées", "Aménagement,\ngestion,\nenvironnement", "Industrie", "Outlier\nInternet", "Infrastructures,\ntransports",  "Culture,\narchitecture", "Tourisme,\nnavigation", "Santé",  "Histoire,\nreligion", "Gouvernement,\narmée,\nconflits", "Education")

for(i in 1:14){ # 14 corresponds to the number of clusters
  plot_cluster(i)
}
```

### 5. Summary of one web page and typologies

We will now summarise the information of the corpus, so that for each web page we obtain a unique cluster. To do that, we will calculate the specifity of the clusters.

```{r}
# retrieve full corpus
corpus$cluster = cutree(dhc, 14)
# and turn it into a df, keep only two variables for the specificity calculation
corpus_df = convert(corpus, to = "data.frame") %>% 
  select(c("segment_source", "cluster")) %>% 
  mutate(cl = paste0("cl_", cluster))

# now calculate get the most specific cluster for each document
spec_doc = tidy_specificities(corpus_df, segment_source, cl) %>% 
# keep only the biggest value for each document
  group_by(segment_source) %>% 
  filter(spec == max(spec)) %>% 
  ungroup() 
# if the specificity is < 2, then it is not significant. Then we will keep the cluster which is the largest.
# if the document contains only one segment, then its cluster doesn't require any calculation. 
max_doc = corpus_df %>% 
  group_by(cl, segment_source) %>% 
  dplyr::summarise(count = n(), .groups = "drop") %>% 
  group_by(segment_source) %>% 
  filter(count == max(count))
max_doc = dplyr::rename(max_doc, cl_max = cl)
max_doc = dplyr::rename(max_doc, count_max = count)

spec = spec_doc %>% 
  left_join(max_doc, by = "segment_source") %>% 
  mutate(cluster = case_when(n == 1 ~ cl_max,
                        spec < 2 ~ cl_max,
                        TRUE ~ cl)) 
# Note: a document can have two clusters. This happens when there is an equality (for example if two segments fall in cluster 1 and two segments fall in cluster 2, the document will be cluster 1 AND 2)
```

Join everything:

```{r join}
# corpus
corpus$cluster = cutree(dhc, 14)
# and turn it into a df, keep only two variables for the specificity calculation
corpus_df = convert(corpus, to = "data.frame") %>% 
  select(c("segment_source", "cluster")) %>% 
  mutate(cl = paste0("cl_", cluster))

# Retrieve the dataset with website typology
df = read.csv("analysis/typology_websites/corpus_website_type.csv") 
typo = df %>% select(c("id", "type"))

# retrieve cluster names
cluster_names = read.csv("input_data/cluster_names.csv", sep = "\t")

# join everything
results = corpus_df %>% 
  left_join(df, by = join_by(segment_source == id))

# summarise the results
res = results %>% 
  group_by(type, cl) %>% 
  dplyr::summarise(count = n(), .groups = "drop") %>% 
  left_join(cluster_names, by = "cl") %>% 
  group_by(type) %>% 
  mutate(perc = count/sum(count)*100) %>% 
  ungroup() 
write.csv(res, "analysis/typology_websites/summary_typology_website_by_cluster.csv")

res_clusters = res %>% 
  # remove websites for which the type has not been identified
  filter(!is.na(type)) %>% 
  group_by(cl) %>% 
  mutate(perc_cl = count/sum(count)*100) %>% 
  ungroup()
```

#### Plot the results

##### Cluster by website

First, let's plot for each website type the distribution of the topics. For example, websites of "academic and scientific" type speak mostly about quality measurements, floods, pollution.

```{r plot_typology_website}
plot_website = function(type_site){
plot = res %>%
  filter(type == type_site) %>% 
  ggplot(aes(x = fct_reorder(name, perc), 
             y = perc, 
             fill = name)) + 
  geom_bar(stat = "identity", 
           position = position_dodge(width = 0.9), 
           width = 0.8, 
           show.legend = FALSE) +
  scale_fill_manual(values = setNames(res$couleur, res$name)) +
  coord_flip() + 
  labs(x = NULL,
       y = "%",
       title = paste0("Répartition des thèmes abordés sur les sites\nde type « ", type_site, " »")) +
  theme_bw(base_family = "CenturySch", 
           base_size = 13)
  
  nom_fichier = str_split(type_site, ",", simplify = TRUE)[,1]
  ggsave(plot, filename = glue::glue("analysis/typology_websites/clusters_by_website/{nom_fichier}.png"), width = 20, height = 15, units = "cm")
  }
```

Run the function

```{r run_typology_website}
list_websites = res %>% 
  distinct(type)

list_websites %>% 
  mutate(data = purrr::map(type, plot_website))
```

##### Website by cluster

Now, let's plot for each cluster the distribution of the website types. For example, for the topic "planning and environment", the websites are mostly institutional.

```{r plot_typology_cluster}
plot_cluster = function(nom, cluster, color){
plot = res_clusters %>%
  filter(cl == cluster) %>% 
  ggplot(aes(x = fct_reorder(type, perc_cl), 
             y = perc_cl)) + 
  geom_bar(fill = color,
           stat = "identity", 
           position = position_dodge(width = 0.9), 
           width = 0.8, 
           show.legend = FALSE) +
  coord_flip() + 
  labs(x = NULL,
       y = "%",
       title = paste0("Répartition des types de site web qui abordent\nle thème « ", nom, " »")) +
  theme_bw(base_family = "CenturySch", 
           base_size = 13)
  
  nom_fichier = str_split(nom, ",", simplify = TRUE)[,1]
  ggsave(plot, filename = glue::glue("analysis/typology_websites/websites_by_cluster/{nom_fichier}.png"), width = 20, height = 15, units = "cm")
  }
```

Run the function:

```{r run_clusters}
list_clusters = res %>% 
  distinct(cl, .keep_all = TRUE) %>% 
  select(c("cl", "name", "couleur")) %>% 
  filter(!is.na(name))

list_clusters %>% 
  mutate(data = purrr::pmap(list(nom = name,
                                 cluster = cl,
                                 color = couleur),
                            plot_cluster))
```
